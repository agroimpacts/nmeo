{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cropland mapping with Unet\n",
    "\n",
    "This notebook trains a Unet convolutional neural network and outputs predictions on PlanetScope NICFI basemap data. It is adapted to run in Google's Colab.\n",
    "\n",
    "You can either upload this notebook or clone the class repo into Colab.  \n",
    "\n",
    "Code in this notebook was developed by Boka Luo and Sam Khallaghi for cropland mapping working conducted by the Agricultural Impacts Research Group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up\n",
    "\n",
    "`rasterio` has to be installed first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "alVq3hMf5jsU",
    "outputId": "1a820fd1-35ad-40d2-ef39-2388aba5eefd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install rasterio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And change the runtime type in Colab to GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WjRZzozl5jsW"
   },
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "# Numerical computation\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "\n",
    "# Structured data wrangling\n",
    "import pandas as pd\n",
    "\n",
    "# Computer vision libraries\n",
    "from sklearn import metrics\n",
    "from skimage import transform as trans\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import gdal\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "\n",
    "# General libraries\n",
    "import itertools\n",
    "from itertools import product\n",
    "import random\n",
    "import math\n",
    "import numbers\n",
    "import copy\n",
    "import os\n",
    "import glob\n",
    "import gc\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import collections\n",
    "from collections import OrderedDict\n",
    "import urllib.parse as urlparse\n",
    "import queue\n",
    "import threading\n",
    "import multiprocessing as mp\n",
    "\n",
    "# DL package\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "from torch.autograd.function import once_differentiable\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.utils as vutils\n",
    "import torch.cuda.comm as comm\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "#from tensorboardX import SummaryWriter\n",
    "\n",
    "# Debugging library for jupyter notebook.\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "# Supress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Magic keywords for Ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect colab notebook to your Google Drive \n",
    "For the first connection you need to follow the pop-up window grab the authorization token and paste it in this cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NK_RCzmp5jsY",
    "outputId": "bb861ccc-6129-47cc-ff8d-891f3feca1b6"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check pytorch, CUDA, GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PyTorch version: {}\".format(torch.__version__))\n",
    "print(\"Cuda version : {}\".format(torch.version.cuda))\n",
    "print('CUDNN version:', torch.backends.cudnn.version())\n",
    "print('Number of available GPU Devices:', torch.cuda.device_count())\n",
    "print(\"current GPU Device: {}\".format(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Seeding\n",
    "\n",
    "Manual seeding makes the model deterministic and keeps the experiments reproducible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ECt3eOm25jsZ"
   },
   "outputs": [],
   "source": [
    "# \"torch.backends.cudnn.benchmark = True\" --> This will allow the cuda backend to optimize your graph during its first execution. \n",
    "# Be aware that if you change the network input/output tensor size the graph will be optimized each time a change occurs.\n",
    "# This can lead to very slow runtime and out of memory errors. Only set this flag if your input and output have always the \n",
    "# same shape. Usually, this results in an improvement of about 20%.\n",
    "\n",
    "def seed_everything(seed = 1234, cudnn = True):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    if cudnn:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "kOVfqL3f5jsa"
   },
   "source": [
    "### Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "wSViTbeg5jsb"
   },
   "outputs": [],
   "source": [
    "### Random Rotation around the center ###\n",
    "def centerRotate(img, label, mask, degree):\n",
    "    '''\n",
    "    Synthesize new image chips by rotating the input chip around its center.\n",
    "    Args:\n",
    "    img (narray): Concatenated variables or brightness value with a dimension of (H, W, C)\n",
    "    label (narray): Ground truth with a dimension of (H,W)\n",
    "    mask (narray): Binary mask represents valid pixels in images and label, in a dimension of (H,W)\n",
    "    degree (tuple or list): Range of degree for rotation\n",
    "    Returns:\n",
    "    (narray, narray, narray) tuple of rotated image, label and mask\n",
    "    '''\n",
    "\n",
    "    if isinstance(degree, tuple) or isinstance(degree, list):\n",
    "        degree = random.uniform(degree[0], degree[1])\n",
    "\n",
    "    # Get the dimensions of the image (e.g. number of rows and columns).\n",
    "    h, w,_ = img.shape\n",
    "\n",
    "    # Determine the image center.\n",
    "    center = (w // 2, h // 2)\n",
    "\n",
    "    # Grab the rotation matrix\n",
    "    rotMtrx = cv2.getRotationMatrix2D(center, degree, 1.0)\n",
    "\n",
    "    # perform the actual rotation for both raw and labeled image.\n",
    "    img = cv2.warpAffine(img, rotMtrx, (w, h))\n",
    "    label = cv2.warpAffine(label, rotMtrx, (w, h))\n",
    "    label = np.rint(label)\n",
    "    mask = cv2.warpAffine(mask, rotMtrx, (w, h))\n",
    "    mask = np.rint(mask)\n",
    "\n",
    "    return img, label, mask\n",
    "\n",
    "### Horizontal, Vertical and Diagonal Flip ###\n",
    "def flip(img, label, mask, ftype):\n",
    "    '''\n",
    "    Synthesize new image chips by flipping the input chip around a user defined \n",
    "    axis.\n",
    "    Args:\n",
    "        img (narray): Concatenated variables or brightness value with a \n",
    "            dimension of (H, W, C)\n",
    "        label (narray): Ground truth with a dimension of (H,W)\n",
    "        mask (narray): Binary mask represents valid pixels in images and \n",
    "            label, in a dimension of (H,W)\n",
    "        ftype (str): Flip type from ['vflip','hflip','dflip']\n",
    "    Returns:\n",
    "        (narray, narray, narray) tuple of flipped image, label and mask\n",
    "    Note:\n",
    "        Provided transformation are:\n",
    "            1) 'vflip', vertical flip\n",
    "            2) 'hflip', horizontal flip\n",
    "            3) 'dflip', diagonal flip\n",
    "    '''\n",
    "\n",
    "    def diagonal_flip(img):\n",
    "        flipped = np.flip(img, 1)\n",
    "        flipped = np.flip(flipped, 0)\n",
    "        return flipped\n",
    "\n",
    "\n",
    "    # Horizontal flip\n",
    "    if ftype == 'hflip':\n",
    "\n",
    "        img = np.flip(img, 0)\n",
    "        label = np.flip(label, 0)\n",
    "        mask = np.flip(mask, 0)\n",
    "\n",
    "    # Vertical flip\n",
    "    elif ftype == 'vflip':\n",
    "\n",
    "        img = np.flip(img, 1)\n",
    "        label = np.flip(label, 1)\n",
    "        mask = np.flip(mask, 1)\n",
    "\n",
    "    # Diagonal flip\n",
    "    elif ftype == 'dflip':\n",
    "\n",
    "        img = diagonal_flip(img)\n",
    "        label = diagonal_flip(label)\n",
    "        mask = diagonal_flip(mask)\n",
    "\n",
    "    else:\n",
    "\n",
    "        raise ValueError(\"Bad flip type\")\n",
    "\n",
    "    return img.copy(), label.copy(), mask.copy()\n",
    "\n",
    "### Random Rescaling of image chips ###\n",
    "def reScale(img, label, mask, scale=(0.8, 1.2), randResizeCrop=False, \n",
    "            diff=False, cenLocate=True):\n",
    "    '''\n",
    "    Synthesize new image chips by rescaling the input chip.\n",
    "    Params:\n",
    "        img (narray): Concatenated variables or brightness value with a \n",
    "            dimension of (H, W, C)\n",
    "        label (narray): Ground truth with a dimension of (H,W)\n",
    "        mask (narray): Binary mask represents valid pixels in images and\n",
    "            label, in a dimension of (H,W)\n",
    "        scale (tuple or list): Range of scale ratio\n",
    "        randResizeCrop (bool): Whether crop the rescaled image chip \n",
    "            randomly or at the center if the chip is larger than inpput ones\n",
    "        diff (bool): Whether change the aspect ratio\n",
    "        cenLocate (bool): Whether locate the rescaled image chip at the center\n",
    "            or a random position if the chip is smaller than input\n",
    "    Returns:\n",
    "        (narray, narray, narray) tuple of rescaled image, label and mask\n",
    "    '''\n",
    "\n",
    "    h, w, _ = img.shape\n",
    "    if isinstance(scale, tuple) or isinstance(scale, list):\n",
    "        resizeH = round(random.uniform(scale[0], scale[1]) * h)\n",
    "        if diff:\n",
    "            resizeW = round(random.uniform(scale[0], scale[1]) * w)\n",
    "        else:\n",
    "            resizeW = resizeH\n",
    "    else:\n",
    "        raise Exception('Wrong scale type!')\n",
    "\n",
    "    imgRe = trans.resize(img, (resizeH, resizeW), preserve_range=True)\n",
    "    labelRe = trans.resize(label, (resizeH, resizeW), preserve_range=True)\n",
    "    maskRe = trans.resize(mask, (resizeH, resizeW), preserve_range=True)\n",
    "\n",
    "\n",
    "    # crop image when length of side is larger than input ones\n",
    "    if randResizeCrop:\n",
    "        x_off = random.randint(0, max(0, resizeH - h))\n",
    "        y_off = random.randint(0, max(0, resizeW - w))\n",
    "    else:\n",
    "        x_off = max(0, (resizeH - h) // 2)\n",
    "        y_off = max(0, (resizeW - w) // 2)\n",
    "\n",
    "    imgRe = imgRe[x_off:x_off + min(h, resizeH), y_off:y_off + min(w, resizeW), :]\n",
    "    labelRe = labelRe[x_off:x_off + min(h, resizeH), y_off:y_off + min(w, resizeW)]\n",
    "    labelRe = np.rint(labelRe)\n",
    "    maskRe = maskRe[x_off:x_off + min(h, resizeH), y_off:y_off + min(w, resizeW)]\n",
    "    maskRe = np.rint(maskRe)\n",
    "\n",
    "    # locate image when it is smaller than input\n",
    "    if resizeH < h or resizeW < w:\n",
    "        if cenLocate:\n",
    "            tlX = max(0, (h - resizeH) // 2)\n",
    "            tlY = max(0, (w - resizeW) // 2)\n",
    "        else:\n",
    "            tlX = random.randint(0, max(0, h - resizeH))\n",
    "            tlY = random.randint(0, max(0, w - resizeW))\n",
    "\n",
    "        # resized result\n",
    "        imgRe, labelRe, maskRe = uniShape(imgRe, labelRe, maskRe, h, tlX, tlY)\n",
    "\n",
    "    return imgRe, labelRe, maskRe\n",
    "\n",
    "### Change pixel brightness to account for atmospheric and ### \n",
    "### illumination noise  ###\n",
    "def shiftBrightness(img, gammaRange=(0.2, 2.0), shiftSubset=(4, 4), \n",
    "                    patchShift=True):\n",
    "    '''\n",
    "    Shift image brightness through gamma correction\n",
    "    Params:\n",
    "        img (narray): Concatenated variables or brightness value with a\n",
    "            dimension of (H, W, C)\n",
    "        gammaRange (tuple): Range of gamma values\n",
    "        shiftSubset (tuple): Number of bands or channels for each shift\n",
    "        patchShift (bool): Whether apply the shift on small patches\n",
    "     Returns:\n",
    "        narray, brightness shifted image\n",
    "    '''\n",
    "\n",
    "\n",
    "    c_start = 0\n",
    "\n",
    "    if patchShift:\n",
    "        for i in shiftSubset:\n",
    "            gamma = random.triangular(gammaRange[0], gammaRange[1], 1)\n",
    "\n",
    "            h, w, _ = img.shape\n",
    "            rotMtrx = cv2.getRotationMatrix2D(\n",
    "                center=(random.randint(0, h), random.randint(0, w)),\n",
    "                angle=random.randint(0, 90), scale=random.uniform(1, 2)\n",
    "            )\n",
    "            mask = cv2.warpAffine(img[:, :, c_start:c_start + i], \n",
    "                                  rotMtrx, (w, h))\n",
    "            mask = np.where(mask, 0, 1)\n",
    "            # apply mask\n",
    "            img_ma = ma.masked_array(img[:, :, c_start:c_start + i], mask=mask)\n",
    "            img[:, :, c_start:c_start + i] = ma.power(img_ma, gamma)\n",
    "            # default extra step -- shift on image\n",
    "            gamma_full = random.triangular(0.5, 1.5, 1)\n",
    "            img[:, :, c_start:c_start + i] = np.power(\n",
    "                img[:, :, c_start:c_start + i], gamma_full\n",
    "            )\n",
    "\n",
    "            c_start += i\n",
    "    else:\n",
    "        # convert image dimension to (C, H, W) if len(img.shape)==3\n",
    "        img = np.transpose(\n",
    "            img, list(range(img.ndim)[-1:]) + list(range(img.ndim)[:-1])\n",
    "        )\n",
    "        for i in shiftSubset:\n",
    "            gamma = random.triangular(gammaRange[0], gammaRange[1], 1)\n",
    "            img[c_start:c_start + i, ] = np.power(\n",
    "                img[c_start:c_start + i, ], gamma\n",
    "            )\n",
    "\n",
    "            c_start += i\n",
    "        img = np.transpose(img, list(range(img.ndim)[-img.ndim + 1:]) + [0])\n",
    "\n",
    "    return img\n",
    "\n",
    "### Unify image and label chips dimensions ###\n",
    "def uniShape(img, label, mask, dsize, tlX=0, tlY=0):\n",
    "\n",
    "    '''\n",
    "    Unify dimension of images and labels to specified data size\n",
    "    Params:\n",
    "    img (narray): Concatenated variables or brightness value with a dimension \n",
    "        of (H, W, C)\n",
    "    label (narray): Ground truth with a dimension of (H,W)\n",
    "    mask (narray): Binary mask represents valid pixels in images and label, \n",
    "        in a dimension of (H,W)\n",
    "    dsize (int): Target data size\n",
    "    tlX (int): Vertical offset by pixels\n",
    "    tlY (int): Horizontal offset by pixels\n",
    "    Returns:\n",
    "    (narray, narray, narray) tuple of shape unified image, label and mask\n",
    "    '''\n",
    "\n",
    "    resizeH, resizeW, c = img.shape\n",
    "\n",
    "    canvas_img = np.zeros((dsize, dsize, c), dtype=img.dtype)\n",
    "    canvas_label = np.zeros((dsize, dsize), dtype=label.dtype)\n",
    "    canvas_mask = np.zeros((dsize, dsize), dtype=label.dtype)\n",
    "\n",
    "    canvas_img[tlX:tlX + resizeH, tlY:tlY + resizeW] = img\n",
    "    canvas_label[tlX:tlX + resizeH, tlY:tlY + resizeW] = label\n",
    "    canvas_mask[tlX:tlX + resizeH, tlY:tlY + resizeW] = mask\n",
    "\n",
    "    return canvas_img, canvas_label, canvas_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "pr6x8ygs5jsd"
   },
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "DgZAedYy5jse"
   },
   "outputs": [],
   "source": [
    "### Load data ####\n",
    "def load_data(dataPath, usage=\"train\", window=None, isLabel=False):\n",
    "    '''\n",
    "    Read geographic data into numpy array\n",
    "    Params:\n",
    "        dataPath (str): Path of data to load\n",
    "        usage (str): Usage of the data: \"train\", \"validate\", or \"predict\"\n",
    "        window (tuple): The view onto a rectangular subset of the data, in the \n",
    "            format of (column offsets, row offsets, width in pixel, \n",
    "            height in pixel)\n",
    "        isLabel (binary): Decide whether to saturate data with tested threshold\n",
    "    Returns:\n",
    "        narray\n",
    "    '''\n",
    "\n",
    "    with rasterio.open(dataPath, \"r\") as src:\n",
    "\n",
    "        if isLabel:\n",
    "            if src.count != 1:\n",
    "                raise InputError(\"Label shape not applicable\")\n",
    "            img = src.read(1)\n",
    "        else:\n",
    "            nodata = src.nodata\n",
    "\n",
    "            if usage in ['train', 'validate']:\n",
    "                # Norm by tile\n",
    "                img = mmNorm(src.read(), nodata=nodata)\n",
    "                img = img[:, max(0, window[1]): window[1] + window[3], \n",
    "                          max(0, window[0]): window[0] + window[2]]\n",
    "\n",
    "            else:\n",
    "                # Norm by tile\n",
    "                img = mmNorm(src.read(), nodata=nodata)\n",
    "\n",
    "    return img\n",
    "\n",
    "### stack images ###\n",
    "def get_stacked_img(imgPaths, usage, window=None):\n",
    "    '''\n",
    "    Read geographic data into numpy array\n",
    "    Params:\n",
    "        gsPath (str): Path of growing season image\n",
    "        osPath (str): Path of off season image\n",
    "        imgPaths (list): List of paths for imgages\n",
    "        usage (str): Usage of the image: \"train\", \"validate\", or \"predict\"\n",
    "        window (tuple): The view onto a rectangular subset of the data, in \n",
    "            the format of (column offsets, row offsets, width in pixel, height in pixel)\n",
    "    Returns:\n",
    "        narray\n",
    "    '''\n",
    "\n",
    "    img_ls = [load_data(m, window=window, usage=usage) for m in imgPaths]\n",
    "    img = np.concatenate(img_ls, axis=0).transpose(1, 2, 0)\n",
    "\n",
    "    if usage in [\"train\", \"validate\"]:\n",
    "        col_off, row_off, col_target, row_target = window\n",
    "        row, col, c = img.shape\n",
    "\n",
    "        if row < row_target or col < col_target:\n",
    "\n",
    "            row_off = abs(row_off) if row_off < 0 else 0\n",
    "            col_off = abs(col_off) if col_off < 0 else 0\n",
    "\n",
    "            canvas = np.zeros((row_target, col_target, c))\n",
    "            canvas[row_off: row_off + row, col_off : col_off + col, :] = img\n",
    "\n",
    "            return canvas\n",
    "\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "    elif usage == \"predict\":\n",
    "        return img\n",
    "\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "### Get buffered window ####\n",
    "\n",
    "def get_buffered_window(srcPath, dstPath, buffer):\n",
    "    '''\n",
    "    Get bounding box representing subset of source image that overlaps with \n",
    "    bufferred destination image, in format of (column offsets, row offsets, \n",
    "    width, height)\n",
    "    Params:\n",
    "        srcPath (str): Path of source image to get subset bounding box\n",
    "        dstPath (str): Path of destination image as a reference to define the\n",
    "            bounding box. Size of the bounding box is (destination width + \n",
    "            buffer * 2, destination height + buffer * 2)\n",
    "        buffer (int): Buffer distance of bounding box edges to destination image \n",
    "            measured by pixel numbers\n",
    "    Returns:\n",
    "        tuple in form of (column offsets, row offsets, width, height)\n",
    "    '''\n",
    "\n",
    "    with rasterio.open(srcPath, \"r\") as src:\n",
    "        gt_src = src.transform\n",
    "\n",
    "    with rasterio.open(dstPath, \"r\") as dst:\n",
    "        gt_dst = dst.transform\n",
    "        w_dst = dst.width\n",
    "        h_dst = dst.height\n",
    "\n",
    "    col_off = round((gt_dst[2] - gt_src[2]) / gt_src[0]) - buffer\n",
    "    row_off = round((gt_dst[5] - gt_src[5]) / gt_src[4]) - buffer\n",
    "    width = w_dst + buffer * 2\n",
    "    height = h_dst + buffer * 2\n",
    "\n",
    "    return col_off, row_off, width, height\n",
    "\n",
    "### Get meta from bounds ###\n",
    "\n",
    "def get_meta_from_bounds(file, buffer):\n",
    "    '''\n",
    "    Get metadata of unbuffered region in given file\n",
    "    Params:\n",
    "        file (str):  File name of a image chip\n",
    "        buffer (int): Buffer distance measured by pixel numbers\n",
    "    Returns:\n",
    "        dictionary\n",
    "    '''\n",
    "\n",
    "    with rasterio.open(file, \"r\") as src:\n",
    "\n",
    "        meta = src.meta\n",
    "        dst_width = src.width - 2 * buffer\n",
    "        dst_height = src.height - 2 * buffer\n",
    "\n",
    "        window = Window(buffer, buffer, dst_width, dst_height)\n",
    "        win_transform = src.window_transform(window)\n",
    "\n",
    "    meta.update({\n",
    "        'width': dst_width,\n",
    "        'height': dst_height,\n",
    "        'transform': win_transform,\n",
    "        'count': 1,\n",
    "        'nodata': -128,\n",
    "        'dtype': 'int8'\n",
    "    })\n",
    "\n",
    "    return meta\n",
    "\n",
    "### Image Normalization ###\n",
    "def mmNorm(img, nodata):\n",
    "    '''\n",
    "    Data normalization with min/max method\n",
    "    Params:\n",
    "        img (narray): The targeted image for normalization\n",
    "    Returns:\n",
    "        narrray\n",
    "    '''\n",
    "\n",
    "    img_tmp = np.where(img == nodata, np.nan, img)\n",
    "    img_max = np.nanmax(img_tmp)\n",
    "    img_min = np.nanmin(img_tmp)\n",
    "    normalized = (img - img_min)/(img_max - img_min)\n",
    "\n",
    "    return normalized\n",
    "\n",
    "### Get Chips ###\n",
    "def get_chips(img, dsize, buffer):\n",
    "    '''\n",
    "    Generate small chips from input images and the corresponding index of each \n",
    "    chip The index marks the location of corresponding upper-left pixel of a \n",
    "    chip.\n",
    "    Params:\n",
    "        img (narray): Image in format of (H,W,C) to be crop, in this case it is \n",
    "            the concatenated image of growing season and off season\n",
    "        dsize (int): Cropped chip size\n",
    "        buffer (int):Number of overlapping pixels when extracting images chips\n",
    "    Returns:\n",
    "        list of cropped chips and corresponding coordinates\n",
    "    '''\n",
    "\n",
    "    h, w, _ = img.shape\n",
    "    x_ls = range(0,h - 2 * buffer, dsize - 2 * buffer)\n",
    "    y_ls = range(0, w - 2 * buffer, dsize - 2 * buffer)\n",
    "\n",
    "    index = list(itertools.product(x_ls, y_ls))\n",
    "\n",
    "    img_ls = []\n",
    "    for i in range(len(index)):\n",
    "        x, y = index[i]\n",
    "        img_ls.append(img[x:x + dsize, y:y + dsize, :])\n",
    "\n",
    "    return img_ls, index\n",
    "\n",
    "### Input Error handling ###\n",
    "class InputError(Exception):\n",
    "    '''\n",
    "    Exception raised for errors in the input\n",
    "    '''\n",
    "\n",
    "    def __init__(self, message):\n",
    "        '''\n",
    "        Params:\n",
    "            message (str): explanation of the error\n",
    "        '''\n",
    "\n",
    "        self.message = message\n",
    "\n",
    "    def __str__(self):\n",
    "        '''\n",
    "        Define message to return when error is raised\n",
    "        '''\n",
    "\n",
    "        if self.message:\n",
    "            return 'InputError, {} '.format(self.message)\n",
    "        else:\n",
    "            return 'InputError'\n",
    "\n",
    "### Polynomial Learning rate Decay Policy ###\n",
    "\n",
    "class PolynomialLR(_LRScheduler):\n",
    "    \"\"\"Polynomial learning rate decay until step reach to max_decay_step\n",
    "    \n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        max_decay_steps: after this step, we stop decreasing learning rate\n",
    "        min_learning_rate: scheduler stoping learning rate decay, value of \n",
    "            learning rate must be this value\n",
    "        power: The power of the polynomial.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer, max_decay_steps, min_learning_rate=1e-5, \n",
    "                 power=1.0):\n",
    "        \n",
    "        if max_decay_steps <= 1.:\n",
    "            raise ValueError('max_decay_steps should be greater than 1.')\n",
    "        \n",
    "        self.max_decay_steps = max_decay_steps\n",
    "        self.min_learning_rate = min_learning_rate\n",
    "        self.power = power\n",
    "        self.last_step = 0\n",
    "        \n",
    "        super().__init__(optimizer)\n",
    "        \n",
    "    def get_lr(self):\n",
    "        if self.last_step > self.max_decay_steps:\n",
    "            return [self.min_learning_rate for _ in self.base_lrs]\n",
    "\n",
    "        return [(base_lr - self.min_learning_rate) * \n",
    "                ((1 - self.last_step / self.max_decay_steps) ** (self.power)) + \n",
    "                self.min_learning_rate for base_lr in self.base_lrs]\n",
    "    \n",
    "    def step(self, step=None):\n",
    "        \n",
    "        if step is None:\n",
    "            step = self.last_step + 1\n",
    "        self.last_step = step if step != 0 else 1\n",
    "        \n",
    "        if self.last_step <= self.max_decay_steps:\n",
    "            decay_lrs = [\n",
    "                (base_lr - self.min_learning_rate) * \n",
    "                ((1 - self.last_step / self.max_decay_steps) ** (self.power)) + \n",
    "                self.min_learning_rate for base_lr in self.base_lrs\n",
    "            ]\n",
    "            \n",
    "            for param_group, lr in zip(self.optimizer.param_groups, decay_lrs):\n",
    "                param_group['lr'] = lr\n",
    "\n",
    "### Reading the dataframe input with parallel workers ###\n",
    "def parallelize_df(df, func, n_cores=os.cpu_count(), **kwargs):\n",
    "    '''\n",
    "    Processes specified method on pandas dataframe using multiple cores\n",
    "    Params:\n",
    "        df (''pd.DataFrames''): Pandas dataframe to be processed\n",
    "        func: Method to apply on provided dataframe\n",
    "        n_cores (int): Number of processes that the mother process splits into\n",
    "    Returns:\n",
    "        ''pd.DataFrames''\n",
    "    '''\n",
    "\n",
    "    n_cores = min(n_cores, len(df))    \n",
    "    other_args = [kwargs['{}'.format(m)] for m in func.__code__.co_varnames[1:]]\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    \n",
    "    pool = mp.Pool(n_cores)\n",
    "    df_map = pool.starmap(func, product(df_split, *[[m] for m in other_args]))\n",
    "\n",
    "    df = pd.concat(df_map)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    return df\n",
    "\n",
    "### Multi-core ###\n",
    "\n",
    "def multicore(func, args, n_cores=os.cpu_count()):\n",
    "    '''\n",
    "    Processes specified method on a series of arguments in parallel. Number of \n",
    "    cores is determined by whichever is smaller of the computer cores or the \n",
    "    arguments nubmer.\n",
    "    Params:\n",
    "        fuc: function to apply on provided arguments\n",
    "        args (list): a list of independent arguments\n",
    "    '''\n",
    "\n",
    "    n_cores = min(n_cores, len(args))\n",
    "    pool = mp.Pool(processes=n_cores)\n",
    "    pool.map(func, args)\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Balanced Cross Entropy Loss ###\n",
    "class BalancedCrossEntropyLoss(nn.Module):\n",
    "    r'''\n",
    "    Cross Entropy loss weighted based on inverse class ratio strategy.\n",
    "    \n",
    "    Arguments:\n",
    "        ignore_index (int) -- Class index to ignore\n",
    "        reduction (str) -- Reduction method to apply, return mean over batch if 'mean',\n",
    "                         return sum if 'sum', return a tensor of shape [N,] if 'none'\n",
    "    Returns:\n",
    "        Loss tensor according to arg reduction\n",
    "    '''\n",
    "\n",
    "    def __init__(self, ignore_index=-100, reduction='mean'):\n",
    "        super(BalancedCrossEntropyLoss, self).__init__()\n",
    "        \n",
    "        self.ignore_index = ignore_index\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, predict, target):\n",
    "        # get class weights\n",
    "        unique, unique_counts = torch.unique(target, return_counts=True)\n",
    "        \n",
    "        # calculate weight for only valid indices\n",
    "        unique_counts = unique_counts[unique != self.ignore_index]\n",
    "        unique = unique[unique != self.ignore_index]\n",
    "        ratio = unique_counts.float() / torch.numel(target)\n",
    "        weight = (1. / ratio) / torch.sum(1. / ratio)\n",
    "\n",
    "        lossWeight = torch.ones(predict.shape[1]).cuda() * 0.00001\n",
    "        \n",
    "        for i in range(len(unique)):\n",
    "            lossWeight[unique[i]] = weight[i]\n",
    "        \n",
    "        loss = nn.CrossEntropyLoss(weight=lossWeight, ignore_index=self.ignore_index, reduction=self.reduction)\n",
    "\n",
    "        return loss(predict, target)\n",
    "\n",
    "### Dice Loss Family ###\n",
    "class BinaryDiceLoss(nn.Module):\n",
    "    r'''\n",
    "    Dice loss of binary classes.\n",
    "    Arguments:\n",
    "        smooth (float): A float number to smooth loss, and avoid NaN error, default: 1\n",
    "        p (int): Denominator value: \\sum{x^p} + \\sum{y^p}, default: 2\n",
    "        predict (torch.tensor): Predicted tensor of shape [N, *]\n",
    "        target (torch.tensor): Target tensor of same shape with predict\n",
    "    Returns:\n",
    "        Loss tensor\n",
    "    '''\n",
    "\n",
    "    def __init__(self, smooth=1, p=1):\n",
    "        super(BinaryDiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, predict, target):\n",
    "        assert predict.shape[0] == target.shape[0], \"predict & target batch size do not match\"\n",
    "        predict = predict.contiguous().view(-1)\n",
    "        target = target.contiguous().view(-1)\n",
    "\n",
    "        num = 2 * (predict * target).sum() + self.smooth\n",
    "        den = (predict.pow(self.p) + target.pow(self.p)).sum() + self.smooth\n",
    "        loss = 1 - num / den\n",
    "\n",
    "        return loss\n",
    "\n",
    "###\n",
    "class DiceLoss(nn.Module):\n",
    "    r'''\n",
    "    Dice loss\n",
    "    \n",
    "    Arguments:\n",
    "        weight (torch.tensor): Weight array of shape [num_classes,]\n",
    "        ignore_index (int): Class index to ignore\n",
    "        predict (torch.tensor): Predicted tensor of shape [N, C, *]\n",
    "        target (torch.tensor): Target tensor either in shape [N,*] or of same shape with predict\n",
    "        other args pass to BinaryDiceLoss\n",
    "    Returns:\n",
    "        same as BinaryDiceLoss\n",
    "    '''\n",
    "\n",
    "    def __init__(self, weight=None, ignore_index=-100, **kwargs):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.kwargs = kwargs\n",
    "        self.weight = weight\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, predict, target):\n",
    "        nclass = predict.shape[1]\n",
    "        if predict.shape == target.shape:\n",
    "            pass\n",
    "        elif len(predict.shape) == 4:\n",
    "            target = F.one_hot(target, num_classes=nclass).permute(0, 3, 1, 2).contiguous()\n",
    "        else:\n",
    "            assert 'Predict tensor shape of {} is not assceptable.'.format(predict.shape)\n",
    "\n",
    "        dice = BinaryDiceLoss(**self.kwargs)\n",
    "        total_loss = 0\n",
    "        weight = torch.Tensor([1. / nclass] * nclass).cuda() if self.weight is None else self.weight\n",
    "        predict = F.softmax(predict, dim=1)\n",
    "\n",
    "        for i in range(nclass):\n",
    "            if i != self.ignore_index:\n",
    "                dice_loss = dice(predict[:, i], target[:, i])\n",
    "\n",
    "                assert weight.shape[0] == nclass, \\\n",
    "                    'Expected weight tensor with shape [{}], but got[{}]'.format(nclass, weight.shape[0])\n",
    "                dice_loss *= weight[i]\n",
    "                total_loss += dice_loss\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "###\n",
    "class BalancedDiceLoss(nn.Module):\n",
    "    r'''\n",
    "    Dice Loss weighted by inverse of label frequency\n",
    "    Arguments:\n",
    "        ignore_index (int): Class index to ignore\n",
    "        predict (torch.tensor): Predicted tensor of shape [N, C, *]\n",
    "        target (torch.tensor): Target tensor either in shape [N,*] or of same shape with predict\n",
    "        other args pass to BinaryDiceLoss\n",
    "    Returns:\n",
    "        same as BinaryDiceLoss\n",
    "    '''\n",
    "\n",
    "    def __init__(self, ignore_index=-100, **kwargs):\n",
    "        super(BalancedDiceLoss, self).__init__()\n",
    "        self.kwargs = kwargs\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, predict, target):\n",
    "        # get class weights\n",
    "        unique, unique_counts = torch.unique(target, return_counts=True)\n",
    "        # calculate weight for only valid indices\n",
    "        unique_counts = unique_counts[unique != self.ignore_index]\n",
    "        unique = unique[unique != self.ignore_index]\n",
    "        ratio = unique_counts.float() / torch.numel(target)\n",
    "        weight = (1. / ratio) / torch.sum(1. / ratio)\n",
    "\n",
    "        lossWeight = torch.ones(predict.shape[1]).cuda() * 0.00001\n",
    "        for i in range(len(unique)):\n",
    "            lossWeight[unique[i]] = weight[i]\n",
    "\n",
    "        loss = DiceLoss(weight=lossWeight, ignore_index=self.ignore_index, **self.kwargs)\n",
    "\n",
    "        return loss(predict, target)\n",
    "\n",
    "###\n",
    "class DiceCELoss(nn.Module):\n",
    "    '''\n",
    "    Combination of dice loss and cross entropy loss through summation\n",
    "    \n",
    "    Arguments:\n",
    "        loss_weight (tensor): a manual rescaling weight given to each class. If given, has to be a Tensor of size C\n",
    "        dice_weight (float): Weight on dice loss for the summation, while weight on cross entropy loss is\n",
    "                             (1 - dice_weight)\n",
    "        dice_smooth (float): A float number to smooth dice loss, and avoid NaN error, default: 1\n",
    "        dice_p (int): Denominator value: \\sum{x^p} + \\sum{y^p}, default: 2\n",
    "        ignore_index (int): Class index to ignore\n",
    "    Returns:\n",
    "        Loss tensor\n",
    "    '''\n",
    "\n",
    "    def __init__(self, loss_weight = None, dice_weight=0.5 , dice_smooth=1, dice_p=1, ignore_index=-100):\n",
    "        super(DiceCELoss, self).__init__()\n",
    "        self.loss_weight = loss_weight\n",
    "        self.dice_weight = dice_weight\n",
    "        self.dice_smooth = dice_smooth\n",
    "        self.dice_p = dice_p\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, predict, target):\n",
    "        assert predict.shape[0] == target.shape[0], \"predict & target batch size do not match\"\n",
    "\n",
    "        dice = DiceLoss(weight=self.loss_weight, ignore_index=self.ignore_index, smooth=self.dice_smooth, p=self.dice_p)\n",
    "        ce = nn.CrossEntropyLoss(weight=self.loss_weight, ignore_index=self.ignore_index)\n",
    "        loss = self.dice_weight * dice(predict, target) + (1 - self.dice_weight) * ce(predict, target)\n",
    "\n",
    "        return loss\n",
    "\n",
    "###\n",
    "class BalancedDiceCELoss(nn.Module):\n",
    "    r'''\n",
    "    Dice Cross Entropy weighted by inverse of label frequency\n",
    "    Arguments:\n",
    "        ignore_index (int): Class index to ignore\n",
    "        predict (torch.tensor): Predicted tensor of shape [N, C, *]\n",
    "        target (torch.tensor): Target tensor either in shape [N,*] or of same shape with predict\n",
    "        other args pass to DiceCELoss, excluding loss_weight\n",
    "    Returns:\n",
    "        Same as DiceCELoss\n",
    "    '''\n",
    "\n",
    "    def __init__(self, ignore_index=-100, **kwargs):\n",
    "        super(BalancedDiceCELoss, self).__init__()\n",
    "        self.ignore_index =  ignore_index\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def forward(self, predict, target):\n",
    "        # get class weights\n",
    "        unique, unique_counts = torch.unique(target, return_counts=True)\n",
    "        # calculate weight for only valid indices\n",
    "        unique_counts = unique_counts[unique != self.ignore_index]\n",
    "        unique = unique[unique != self.ignore_index]\n",
    "        ratio = unique_counts.float() / torch.numel(target)\n",
    "        weight = (1. / ratio) / torch.sum(1. / ratio)\n",
    "\n",
    "        lossWeight = torch.ones(predict.shape[1]).cuda() * 0.00001\n",
    "        for i in range(len(unique)):\n",
    "            lossWeight[unique[i]] = weight[i]\n",
    "\n",
    "        loss = DiceCELoss(loss_weight=lossWeight, **self.kwargs)\n",
    "\n",
    "        return loss(predict, target)\n",
    "\n",
    "### Tversky-Focal Loss Family ###\n",
    "class BinaryTverskyFocalLoss(nn.Module):\n",
    "    r'''\n",
    "    Pytorch versiono of tversky focal loss proposed in paper\n",
    "    'A novel focal Tversky loss function and improved Attention U-Net for lesion segmentation'\n",
    "    (https://arxiv.org/abs/1810.07842)\n",
    "    \n",
    "    Arguments:\n",
    "        smooth (float): A float number to smooth loss, and avoid NaN error, default: 1\n",
    "        alpha (float): Hyperparameters alpha, paired with (1 - alpha) to shift emphasis to improve recall\n",
    "        gamma (float): Tversky index, default: 1.33\n",
    "        predict (torch.tensor): Predicted tensor of shape [N, C, *]\n",
    "        target (torch.tensor): Target tensor either in shape [N,*] or of same shape with predict\n",
    "    \n",
    "    Returns:\n",
    "        Loss tensor\n",
    "    '''\n",
    "\n",
    "    def __init__(self, smooth=1, alpha=0.7, gamma=1.33):\n",
    "        super(BinaryTverskyFocalLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        self.alpha = alpha\n",
    "        self.beta = 1 - self.alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "\n",
    "    def forward(self, predict, target):\n",
    "        assert predict.shape[0] == target.shape[0], \"predict & target batch size do not match\"\n",
    "\n",
    "        # no reduction, same as original paper\n",
    "        predict = predict.contiguous().view(-1)\n",
    "        target = target.contiguous().view( -1)\n",
    "\n",
    "        num = (predict * target).sum() + self.smooth\n",
    "        den = (predict * target).sum() + self.alpha * ((1 - predict) * target).sum() \\\n",
    "              + self.beta * (predict * (1 - target)).sum() + self.smooth\n",
    "        loss = torch.pow(1 - num/den, 1 / self.gamma)\n",
    "\n",
    "        return loss\n",
    "            \n",
    "###\n",
    "class TverskyFocalLoss(nn.Module):\n",
    "    r'''\n",
    "    Tversky focal loss\n",
    "    \n",
    "    Arguments:\n",
    "        weight (torch.tensor): Weight array of shape [num_classes,]\n",
    "        ignore_index (int): Class index to ignore\n",
    "        predict (torch.tensor): Predicted tensor of shape [N, C, *]\n",
    "        target (torch.tensor): Target tensor either in shape [N,*] or of same shape with predict\n",
    "        other args pass to BinaryTverskyFocalLoss\n",
    "    Returns:\n",
    "        same as BinaryTverskyFocalLoss\n",
    "    '''\n",
    "    def __init__(self, weight=None, ignore_index=-100, **kwargs):\n",
    "        super(TverskyFocalLoss, self).__init__()\n",
    "        self.kwargs = kwargs\n",
    "        self.weight = weight\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, predict, target):\n",
    "        nclass = predict.shape[1]\n",
    "        if predict.shape == target.shape:\n",
    "            pass\n",
    "        elif len(predict.shape) == 4:\n",
    "            target = F.one_hot(target, num_classes=nclass).permute(0, 3, 1, 2).contiguous()\n",
    "        else:\n",
    "            assert 'predict shape not applicable'\n",
    "\n",
    "        tversky = BinaryTverskyFocalLoss(**self.kwargs)\n",
    "        total_loss = 0\n",
    "        weight = torch.Tensor([1./nclass] * nclass).cuda() if self.weight is None else self.weight\n",
    "        predict = F.softmax(predict, dim=1)\n",
    "        \n",
    "        for i in range(nclass):\n",
    "            if i != self.ignore_index:\n",
    "                tversky_loss = tversky(predict[:, i], target[:, i])\n",
    "                assert weight.shape[0] == nclass, \\\n",
    "                    'Expect weight shape [{}], get[{}]'.format(nclass, weight.shape[0])\n",
    "                tversky_loss *= weight[i]\n",
    "                total_loss += tversky_loss\n",
    "            \n",
    "        return total_loss\n",
    "\n",
    "###\n",
    "class BalancedTverskyFocalLoss(nn.Module):\n",
    "    r'''  \n",
    "    Tversky focal loss weighted by inverse of label frequency\n",
    "    Arguments:\n",
    "        ignore_index (int): Class index to ignore\n",
    "        predict (torch.tensor): Predicted tensor of shape [N, C, *]\n",
    "        target (torch.tensor): Target tensor either in shape [N,*] or of same shape with predict\n",
    "        other args pass to BinaryTverskyFocalLoss\n",
    "    Returns:\n",
    "        same as TverskyFocalLoss\n",
    "    '''\n",
    "\n",
    "    def __init__(self, ignore_index=-100, **kwargs):\n",
    "        super(BalancedTverskyFocalLoss, self).__init__()\n",
    "        self.kwargs = kwargs\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, predict, target):\n",
    "        # get class weights\n",
    "        unique, unique_counts = torch.unique(target, return_counts=True)\n",
    "        # calculate weight for only valid indices\n",
    "        unique_counts = unique_counts[unique != self.ignore_index]\n",
    "        unique = unique[unique != self.ignore_index]\n",
    "        ratio = unique_counts.float() / torch.numel(target)\n",
    "        weight = (1. / ratio) / torch.sum(1. / ratio)\n",
    "\n",
    "        lossWeight = torch.ones(predict.shape[1]).cuda() * 0.00001\n",
    "        for i in range(len(unique)):\n",
    "                lossWeight[unique[i]] = weight[i]\n",
    "\n",
    "        # loss\n",
    "        loss = TverskyFocalLoss(weight=lossWeight, ignore_index=self.ignore_index, **self.kwargs)\n",
    "\n",
    "        return loss(predict, target)\n",
    "\n",
    "###\n",
    "class TverskyFocalCELoss(nn.Module):\n",
    "    '''\n",
    "    Combination of tversky focal loss and cross entropy loss though summation\n",
    "    Arguments:\n",
    "        loss_weight (tensor): a manual rescaling weight given to each class. If given, has to be a Tensor of size C\n",
    "        tversky_weight (float): Weight on tversky focal loss for the summation, while weight on cross entropy loss\n",
    "                                is (1 - tversky_weight)\n",
    "        tversky_smooth (float): A float number to smooth tversky focal loss, and avoid NaN error, default: 1\n",
    "        tversky_alpha (float):\n",
    "        tversky_gamma (float):\n",
    "        ignore_index (int): Class index to ignore\n",
    "    Returns:\n",
    "        Loss tensor\n",
    "    '''\n",
    "\n",
    "    def __init__(self, loss_weight=None, tversky_weight=0.5, tversky_smooth=1, tversky_alpha=0.7, \n",
    "                 tversky_gamma=0.9, ignore_index=-100):\n",
    "        super(TverskyFocalCELoss, self).__init__()\n",
    "        self.loss_weight = loss_weight\n",
    "        self.tversky_weight = tversky_weight\n",
    "        self.tversky_smooth = tversky_smooth\n",
    "        self.tversky_alpha = tversky_alpha\n",
    "        self.tversky_gamma = tversky_gamma\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, predict, target):\n",
    "        assert predict.shape[0] == target.shape[0], \"predict & target batch size do not match\"\n",
    "\n",
    "        tversky = TverskyFocalLoss(weight=self.loss_weight, ignore_index=self.ignore_index, smooth=self.tversky_smooth,\n",
    "                                   alpha=self.tversky_alpha, gamma=self.tversky_gamma)\n",
    "        ce = nn.CrossEntropyLoss(weight=self.loss_weight, ignore_index=self.ignore_index)\n",
    "        loss = self.tversky_weight * tversky(predict, target) + (1 - self.tversky_weight) * ce(predict, target)\n",
    "\n",
    "        return loss\n",
    "\n",
    "###\n",
    "class BalancedTverskyFocalCELoss(nn.Module):\n",
    "    r'''\n",
    "    Combination of tversky focal loss and cross entropy loss weighted by inverse of label frequency\n",
    "    \n",
    "    Arguments:\n",
    "        ignore_index (int): Class index to ignore\n",
    "        predict (torch.tensor): Predicted tensor of shape [N, C, *]\n",
    "        target (torch.tensor): Target tensor either in shape [N,*] or of same shape with predict\n",
    "        other args pass to DiceCELoss, excluding loss_weight\n",
    "    Returns:\n",
    "        Same as TverskyFocalCELoss\n",
    "    '''\n",
    "\n",
    "    def __init__(self, ignore_index=-100, **kwargs):\n",
    "        super(BalancedTverskyFocalCELoss, self).__init__()\n",
    "        self.ignore_index =  ignore_index\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def forward(self, predict, target):\n",
    "        # get class weights\n",
    "        unique, unique_counts = torch.unique(target, return_counts=True)\n",
    "        # calculate weight for only valid indices\n",
    "        unique_counts = unique_counts[unique != self.ignore_index]\n",
    "        unique = unique[unique != self.ignore_index]\n",
    "        ratio = unique_counts.float() / torch.numel(target)\n",
    "        weight = (1. / ratio) / torch.sum(1. / ratio)\n",
    "\n",
    "        lossWeight = torch.ones(predict.shape[1]).cuda() * 0.00001\n",
    "        for i in range(len(unique)):\n",
    "            lossWeight[unique[i]] = weight[i]\n",
    "\n",
    "        loss = TverskyFocalCELoss(loss_weight=lossWeight, **self.kwargs)\n",
    "\n",
    "        return loss(predict, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "FVoXfxJw5jsg"
   },
   "source": [
    "### Accuracy Metrics + Evaluation \n",
    "\n",
    "Performend on the validation dataset, and producing a CSV report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true,
    "id": "CF-53D2s5jsh"
   },
   "outputs": [],
   "source": [
    "class BinaryMetrics:\n",
    "    '''\n",
    "    Metrics measuring model performance.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, refArray, scoreArray, predArray=None):\n",
    "        '''\n",
    "        Params:\n",
    "            refArray (narray): Array of ground truth\n",
    "            scoreArray (narray): Array of pixels scores of positive class\n",
    "            predArray (narray): Boolean array of predictions telling whether a pixel belongs to a specific class\n",
    "        '''\n",
    "\n",
    "        self.eps = 10e-6\n",
    "        self.observation = refArray.flatten()\n",
    "        self.score = scoreArray.flatten()\n",
    "        if predArray is not None:\n",
    "            self.prediction = predArray.flatten()\n",
    "        # take score over 0.5 as prediction if predArray not provided\n",
    "        else:\n",
    "            self.prediction = np.where(self.score > 0.5, 1, 0)\n",
    "        self.confusion_matrix = self.confusion_matrix()\n",
    "\n",
    "        if self.observation.shape != self.score.shape:\n",
    "            raise InputError(\"Inconsistent input shape\")\n",
    "\n",
    "    def __add__(self, other):\n",
    "        \"\"\"\n",
    "        Add two BinaryMetrics instances\n",
    "        Params:\n",
    "            other (''BinaryMetrics''): A BinaryMetrics instance\n",
    "        Return:\n",
    "            ''BinaryMetrics''\n",
    "        \"\"\"\n",
    "\n",
    "        return BinaryMetrics(np.append(self.observation, other.observation),\n",
    "                             np.append(self.score, other.score),\n",
    "                            np.append(self.prediction, other.prediction))\n",
    "\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        \"\"\"\n",
    "        Add a BinaryMetrics instance with reversed operands\n",
    "        Params:\n",
    "            other\n",
    "        Returns:\n",
    "            ''BinaryMetrics\n",
    "        \"\"\"\n",
    "\n",
    "        if other == 0:\n",
    "            return self\n",
    "        else:\n",
    "            return self.__add__(other)\n",
    "\n",
    "\n",
    "    def confusion_matrix(self):\n",
    "        \"\"\"\n",
    "        Calculate confusion matrix of given ground truth and predicted label\n",
    "        Returns:\n",
    "            ''pandas.dataframe'' of observation on the column and prediction on the row\n",
    "        \"\"\"\n",
    "\n",
    "        refArray = self.observation\n",
    "        predArray = self.prediction\n",
    "\n",
    "        if refArray.max() > 1 or predArray.max() > 1:\n",
    "            raise Exception(\"Invalid array\")\n",
    "        predArray = predArray * 2\n",
    "        sub = refArray - predArray\n",
    "\n",
    "        self.tp = np.sum(sub == -1)\n",
    "        self.fp = np.sum(sub == -2)\n",
    "        self.fn = np.sum(sub == 1)\n",
    "        self.tn = np.sum(sub == 0)\n",
    "\n",
    "        confusionMatrix = pd.DataFrame(data=np.array([[self.tn, self.fp], [self.fn, self.tp]]),\n",
    "                                       index=['observation = 0', 'observation = 1'],\n",
    "                                       columns = ['prediction = 0', 'prediction = 1'])\n",
    "        return confusionMatrix\n",
    "\n",
    "\n",
    "    def iou(self):\n",
    "        \"\"\"\n",
    "        Calculate interception over union\n",
    "        Returns:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        return metrics.jaccard_score(self.observation, self.prediction)\n",
    "\n",
    "\n",
    "    def precision(self):\n",
    "        \"\"\"\n",
    "        Calculate precision\n",
    "        Returns:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        return metrics.precision_score(self.observation, self.prediction)\n",
    "\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"\n",
    "        Calculate recall\n",
    "        Returns:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        return metrics.recall_score(self.observation, self.prediction)\n",
    "\n",
    "\n",
    "    def accuracy(self):\n",
    "        \"\"\"\n",
    "        Calculate accuracy\n",
    "        Returns:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        return metrics.accuracy_score(self.observation, self.prediction)\n",
    "\n",
    "\n",
    "    def tss(self):\n",
    "        \"\"\"\n",
    "        Calculate true scale statistic (TSS)\n",
    "        Returns:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        return self.tp / (self.tp + self.fn) + self.tn / (self.tn + self.fp) - 1\n",
    "\n",
    "\n",
    "    def false_positive_rate(self):\n",
    "        \"\"\"\n",
    "        Calculate false positive rate\n",
    "        Returns:\n",
    "             float\n",
    "        \"\"\"\n",
    "\n",
    "        return self.fp / (self.tn + self.fp)\n",
    "\n",
    "    def F1_measure(self):\n",
    "        \"\"\"\n",
    "        Calculate F1 score.\n",
    "        Returns:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            precision = self.tp / (self.tp + self.fp)\n",
    "            recall = self.tp / (self.tp + self.fn)\n",
    "            f1 = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "        except ZeroDivisionError:\n",
    "            precision = self.tp / (self.tp + self.fp + self.eps)\n",
    "            recall = self.tp / (self.tp + self.fn + self.eps)\n",
    "            f1 = (2 * precision * recall) / (precision + recall + self.eps)\n",
    "\n",
    "        return f1\n",
    "\n",
    "\n",
    "    def area_under_roc(self):\n",
    "        \"\"\"\n",
    "        Compute Area Under the Curve (AUC)\n",
    "        Returns:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        return metrics.roc_auc_score(self.observation, self.score)\n",
    "    \n",
    "##################################################\n",
    "\n",
    "def evaluate(evalData, model, buffer, gpu, csv_fn):\n",
    "    \"\"\"\n",
    "    Evaluate model\n",
    "    Params:\n",
    "        evalData (''DataLoader''): Batch grouped data\n",
    "        model: Trained model for validation\n",
    "        buffer: Buffer added to the targeted grid when creating dataset. This allows metrics to calculate only\n",
    "            at non-buffered region\n",
    "        gpu (binary,optional): Decide whether to use GPU, default is True\n",
    "        csv_fn (str): filename to save metrics\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    metrics = []\n",
    "\n",
    "    for img, label in evalData:\n",
    "        img = Variable(img, requires_grad=False)\n",
    "        label = Variable(label, requires_grad=False)\n",
    "\n",
    "        # GPU setting\n",
    "        if gpu:\n",
    "            img = img.cuda()\n",
    "            label = label.cuda()\n",
    "        out = model(img)\n",
    "\n",
    "        # Compute metrics\n",
    "        out = F.softmax(out, 1)\n",
    "        batch, nclass, height, width = out.size()\n",
    "\n",
    "        for i in range(batch):\n",
    "            label_batch = label[i, buffer:-buffer, buffer:-buffer].cpu().numpy()\n",
    "            batch_predict = out.max(dim=1)[1][:, buffer:-buffer, buffer:-buffer].data[i].cpu().numpy()\n",
    "            for n in range(1, nclass):\n",
    "                class_out = out[:, n, buffer:-buffer, buffer:-buffer].data[i].cpu().numpy()\n",
    "                class_predict = np.where(batch_predict == n, 1, 0)\n",
    "                class_label = np.where(label_batch == n, 1, 0)\n",
    "                metrics_chip = BinaryMetrics(class_label, class_out, class_predict)\n",
    "                # append if exists\n",
    "                try:\n",
    "                    metrics[n - 1].append(metrics_chip)\n",
    "                except:\n",
    "                    metrics.append([metrics_chip])\n",
    "\n",
    "    metrics = [sum(m) for m in metrics]\n",
    "    report = pd.DataFrame({\n",
    "        'tss': [m.tss() for m in metrics],\n",
    "        'accuracy': [m.accuracy() for m in metrics],\n",
    "        'precision': [m.precision() for m in metrics],\n",
    "        'recall': [m.recall() for m in metrics],\n",
    "        'fpr': [m.false_positive_rate() for m in metrics],\n",
    "        'F1-score': [m.F1_measure() for m in metrics],\n",
    "        'IoU': [m.iou() for m in metrics],\n",
    "        'AUC': [m.area_under_roc() for m in metrics]\n",
    "    }, index=[\"class_{}\".format(m) for m in range(1, len(metrics) + 1)])\n",
    "    \n",
    "    print(report)\n",
    "    with open(csv_fn, 'w', encoding = 'utf-8-sig') as f:\n",
    "      report.to_csv(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "tmLp37p-5jsi"
   },
   "source": [
    "### Custom Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true,
    "id": "ycikjdqa5jsi"
   },
   "outputs": [],
   "source": [
    "class planetData(Dataset):\n",
    "    '''\n",
    "    Dataset of planet scope image files for pytorch architecture\n",
    "    '''\n",
    "\n",
    "    def __init__(self, root_dir, catalog, dataSize, buffer, bufferComp, usage, imgPathCols, labelPathCol=None,\n",
    "                 labelGroup = [0,1,2,3,4], catalogIndex=None, deRotate=(-90, 90), bShiftSubs=(4, 4), trans=None):\n",
    "\n",
    "        '''\n",
    "        Params:\n",
    "            root_dir (str): Directory storing files of variables and labels\n",
    "            catalog (Pandas.DataFrame): Pandas dataframe giving the list of data and their directories\n",
    "            dataSize (int): Size of chips that is not buffered, i.e., the size of labels\n",
    "            buffer (int): Distance to target chips' boundaries measured by number of pixels when extracting images\n",
    "                (variables), i.e., variables size would be (dsize + buffer) x (dsize + buffer)\n",
    "            bufferComp (int): Buffer used when creating composite. In the case of Ghana, it is 11.\n",
    "            usage (str): Usage of the dataset : \"train\", \"validate\" or \"predict\"\n",
    "            imgPathCols (list): Column names in the catalog referring to image paths\n",
    "            labelPathCol(str): Column name in the catalog referring to label paths\n",
    "            labelGroup (list): Group indices of labels to load, where each group corresponds to a specific level of label quality\n",
    "            catalogIndex (int or None): Row index in catalog to load data for prediction. Only need to be specified when\n",
    "                usage is \"prediction\"\n",
    "            deRotate (tuple or None): Range of degrees for rotation\n",
    "            bShiftSubs (tuple or list): Number of bands or channels on dataset for each brightness shift\n",
    "            trans (list): Data augmentation methods: one or multiple elements from ['vflip','hflip','dflip', 'rotate',\n",
    "                'resize']\n",
    "        Note:\n",
    "            Provided transformation are:\n",
    "                1) 'vflip', vertical flip\n",
    "                2) 'hflip', horizontal flip\n",
    "                3) 'dflip', diagonal flip\n",
    "                4) 'rotate', rotation\n",
    "                5) 'resize', rescale image fitted into the specified data size\n",
    "                6) 'shift_brightness', shift brightness of images\n",
    "            Any value out of the range would cause an error\n",
    "        Note:\n",
    "            Catalog for train and validate contrains at least columns for image path, label path and \"usage\".\n",
    "            Catalog for prediction contains at least columns for image path, \"tile_col\", and \"tile_row\", where the\n",
    "            \"tile_col\" and \"tile_row\" is the relative tile location for naming predictions in Learner\n",
    "        '''\n",
    "\n",
    "        self.buffer = buffer\n",
    "        self.composite_buffer = bufferComp\n",
    "        self.data_size = dataSize\n",
    "        self.chip_size = self.data_size+ self.buffer * 2\n",
    "\n",
    "        self.usage = usage\n",
    "        self.deRotate = deRotate\n",
    "        self.bshift_subs = bShiftSubs\n",
    "        self.trans = trans\n",
    "\n",
    "        self.data_path = root_dir\n",
    "        self.img_cols = imgPathCols if isinstance(imgPathCols, list) else [imgPathCols]\n",
    "        self.label_col = labelPathCol\n",
    "\n",
    "        if self.usage == \"train\":\n",
    "            self.catalog = catalog.loc[(catalog['usage'] == self.usage) &\n",
    "                                       (catalog['label_group'].isin(labelGroup))]\n",
    "            self.img, self.label = self.get_train_validate_data()\n",
    "            print('-------------{} samples loaded in training dataset-----------'.format(len(self.img)))\n",
    "\n",
    "        elif self.usage == \"validate\":\n",
    "            self.catalog = catalog.loc[(catalog['usage'] == self.usage) &\n",
    "                                       (catalog['label_group'].isin(labelGroup))]\n",
    "            self.img, self.label = self.get_train_validate_data()\n",
    "            print('-------------{} samples loaded in validation dataset-----------'.format(len(self.img)))\n",
    "\n",
    "        elif self.usage == \"predict\":\n",
    "            self.catalog = catalog.iloc[catalogIndex]\n",
    "            self.tile = (self.catalog['tile_col'], self.catalog['tile_row'])\n",
    "            self.img, self.index, self.meta = self.get_predict_data()\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Bad usage value\")\n",
    "\n",
    "\n",
    "    def get_train_validate_data(self):\n",
    "        '''\n",
    "        Get paris of image, label for train and validation\n",
    "        Returns:\n",
    "            tuple of list of images and label\n",
    "        '''\n",
    "\n",
    "        def load_label(row, data_path):\n",
    "\n",
    "            buffer = self.buffer\n",
    "\n",
    "            dir_label = row[self.label_col] if row[self.label_col].startswith(\"s3\") \\\n",
    "                else os.path.join(data_path, row[self.label_col])\n",
    "            label = load_data(dir_label, isLabel=True)\n",
    "            label = np.pad(label, buffer, 'constant')\n",
    "\n",
    "            return label\n",
    "\n",
    "        def load_img(row, data_path):\n",
    "\n",
    "            buffer = self.buffer\n",
    "\n",
    "            dir_label = row['dir_label'] if row['dir_label'].startswith(\"s3\") \\\n",
    "                else os.path.join(data_path, row['dir_label'])\n",
    "            dir_imgs = [row[m] if row[m].startswith(\"s3\") else os.path.join(data_path, row[m]) for m in self.img_cols]\n",
    "            window = get_buffered_window(dir_imgs[0], dir_label, buffer)\n",
    "            img = get_stacked_img(dir_imgs, self.usage, window=window)\n",
    "\n",
    "            return img\n",
    "\n",
    "        global list_data # Local function not applicable in parallelism\n",
    "        def list_data(catalog, data_path):\n",
    "\n",
    "            catalog[\"img\"] = catalog.apply(lambda row: load_img(row, data_path), axis=1)\n",
    "            catalog[\"label\"] = catalog.apply(lambda row: load_label(row, data_path), axis=1)\n",
    "\n",
    "            return catalog.filter(items=['label', 'img'])\n",
    "\n",
    "        catalog = parallelize_df(self.catalog, list_data, data_path = self.data_path)\n",
    "\n",
    "        img_ls = catalog['img'].tolist()\n",
    "        label_ls = catalog['label'].tolist()\n",
    "\n",
    "        return img_ls, label_ls\n",
    "\n",
    "\n",
    "\n",
    "    def get_predict_data(self):\n",
    "        '''\n",
    "        Get data for prediction\n",
    "        Returns:\n",
    "            list of cropped chips\n",
    "            list of index representing location of each chip in tile\n",
    "            dictionary of metadata of score map reconstructed from chips\n",
    "        '''\n",
    "\n",
    "        dir_imgs = [self.catalog[m] if self.catalog[m].startswith(\"s3\") \\\n",
    "            else os.path.join(self.data_path, self.catalog[m]) for m in self.img_cols]\n",
    "        img = get_stacked_img(dir_imgs, self.usage)  # entire composite image in (H, W, C)\n",
    "        buffer_diff = self.buffer - self.composite_buffer\n",
    "        h,w,c = img.shape\n",
    "\n",
    "        if buffer_diff > 0:\n",
    "            canvas = np.zeros((h + buffer_diff * 2, w + buffer_diff * 2, c))\n",
    "\n",
    "            for i in range(c):\n",
    "                canvas[:,:,i] = np.pad(img[:,:,i], buffer_diff, mode='reflect')\n",
    "            img = canvas\n",
    "\n",
    "        else:\n",
    "            img = img[buffer_diff:h-buffer_diff, buffer_diff:w-buffer_diff, :]\n",
    "\n",
    "        meta = get_meta_from_bounds(dir_imgs[0], self.composite_buffer) # meta of composite buffer removed\n",
    "        img_ls, index_ls = get_chips(img, self.chip_size, self.buffer)\n",
    "\n",
    "        return img_ls, index_ls, meta\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Support dataset indexing and apply transformation\n",
    "        Args:\n",
    "            index -- Index of each small chips in the dataset\n",
    "        Returns:\n",
    "            tuple\n",
    "        \"\"\"\n",
    "\n",
    "        if self.usage in [\"train\", \"validate\"]:\n",
    "            img = self.img[index]\n",
    "            label = self.label[index]\n",
    "\n",
    "\n",
    "            if self.usage == \"train\":\n",
    "                mask = np.pad(np.ones((self.data_size, self.data_size)), self.buffer, 'constant')\n",
    "                trans = self.trans\n",
    "                # trans = None\n",
    "                deRotate = self.deRotate\n",
    "\n",
    "                if trans:\n",
    "\n",
    "                    # 0.5 possibility to flip\n",
    "                    trans_flip_ls = [m for m in trans if 'flip' in m]\n",
    "                    if random.randint(0, 1) and len(trans_flip_ls) > 1:\n",
    "                        trans_flip = random.sample(trans_flip_ls, 1)\n",
    "                        img, label, mask = flip(img, label, mask, trans_flip[0])\n",
    "\n",
    "                    # 0.5 possibility to resize\n",
    "                    if random.randint(0, 1) and 'resize' in trans:\n",
    "                        img, label, mask = reScale(img, label.astype(np.uint8), mask.astype(np.uint8),\n",
    "                                                   randResizeCrop=True, diff=True, cenLocate=False)\n",
    "\n",
    "                    # 0.5 possibility to rotate\n",
    "                    if random.randint(0, 1) and 'rotate' in trans:\n",
    "                        img, label, mask = centerRotate(img, label, mask, deRotate)\n",
    "\n",
    "                    # 0.5 possibility to shift brightness\n",
    "                    if random.randint(0, 1) and 'shift_brightness' in trans:\n",
    "                        img = shiftBrightness(img, gammaRange = (0.2, 2), shiftSubset = self.bshift_subs, patchShift=True)\n",
    "\n",
    "                # numpy to torch\n",
    "                label = torch.from_numpy(label).long()\n",
    "                mask = torch.from_numpy(mask).long()\n",
    "                img = torch.from_numpy(img.transpose((2, 0, 1))).float()\n",
    "\n",
    "                # display(img[:, self.buffer:-self.buffer, self.buffer:-self.buffer], label[self.buffer:-self.buffer,self.buffer:-self.buffer], mask[self.buffer:-self.buffer,self.buffer:-self.buffer])\n",
    "                # display(img, label, mask)\n",
    "\n",
    "                return img, label, mask\n",
    "\n",
    "            else:\n",
    "                # numpy to torch\n",
    "                label = torch.from_numpy(label).long()\n",
    "                img = torch.from_numpy(img.transpose((2, 0, 1))).float()\n",
    "\n",
    "                return img, label\n",
    "\n",
    "        else:\n",
    "\n",
    "            img = self.img[index]\n",
    "            index = self.index[index]\n",
    "\n",
    "            img = torch.from_numpy(img.transpose((2, 0, 1))).float()\n",
    "\n",
    "            return img, index\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Get size of the dataset\n",
    "        '''\n",
    "\n",
    "        return len(self.img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "uDEbVnQk5jsj"
   },
   "source": [
    "### Model Architecture - Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true,
    "id": "KH9LJVfV5jsj"
   },
   "outputs": [],
   "source": [
    "class Conv3x3_bn_relu(nn.Module):\n",
    "    def __init__(self, inch, outch, padding=0, stride=1, dilation=1, groups=1, relu=True):\n",
    "        super(Conv3x3_bn_relu, self).__init__()\n",
    "        self.applyRelu = relu\n",
    "\n",
    "        self.conv = nn.Sequential(nn.Conv2d(inch, outch, 3, padding=padding, stride=stride, dilation=dilation, groups=groups),\n",
    "                                  nn.BatchNorm2d(outch))\n",
    "        if self.applyRelu:\n",
    "            self.relu = nn.ReLU(True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        if self.applyRelu:\n",
    "            out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "#########################\n",
    "\n",
    "class Conv1x1_bn_relu(nn.Module):\n",
    "    def __init__(self, inch, outch, stride=1, padding=0, dilation=1, groups=1, relu=True):\n",
    "        super(Conv1x1_bn_relu, self).__init__()\n",
    "        self.applyRelu = relu\n",
    "        self.conv = nn.Sequential(nn.Conv2d(inch, outch, 1, stride=stride, padding=padding, dilation=dilation, groups=groups),\n",
    "                                  nn.BatchNorm2d(outch))\n",
    "\n",
    "        if self.applyRelu:\n",
    "            self.relu = nn.ReLU(True)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x.clone())\n",
    "        if self.applyRelu:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "#########################\n",
    "# Consecutive 2 convolution with batch normalization and ReLU activation\n",
    "class doubleConv(nn.Module):\n",
    "    def __init__(self, inch, outch):\n",
    "        super(doubleConv, self).__init__()\n",
    "        self.conv1 = Conv3x3_bn_relu(inch, outch, padding=1)\n",
    "        self.conv2 = Conv3x3_bn_relu(outch, outch, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "##################################################\n",
    "\n",
    "# unet construction\n",
    "class unet(nn.Module):\n",
    "    def __init__(self, inch, classNum):\n",
    "        super(unet, self).__init__()\n",
    "        # downsample\n",
    "        self.dlyr1 = doubleConv(inch, 64)\n",
    "        self.ds = nn.MaxPool2d(2, stride=2)\n",
    "        self.dlyr2 = doubleConv(64, 128)\n",
    "        self.dlyr3 = doubleConv(128, 256)\n",
    "        self.dlyr4 = doubleConv(256, 512)\n",
    "        self.dlyr5 = doubleConv(512, 1024)\n",
    "        self.dlyr6 = doubleConv(1024, 2048)\n",
    "\n",
    "        # upsample\n",
    "        self.us_init = nn.ConvTranspose2d(2048, 1024, 4, stride=2, padding=1)\n",
    "        self.ulyr_init = doubleConv(2048, 1024)\n",
    "        self.us6 = nn.ConvTranspose2d(1024, 512, 4, stride=2, padding=1)\n",
    "        self.ulyr6 = doubleConv(1024, 512)  # 512x32x32\n",
    "        self.us7 = nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1)\n",
    "        self.ulyr7 = doubleConv(512, 256)\n",
    "        self.us8 = nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1)\n",
    "        self.ulyr8 = doubleConv(256, 128)\n",
    "        self.us9 = nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1)\n",
    "        self.ulyr9 = doubleConv(128, 64)\n",
    "        self.dimTrans = nn.Conv2d(64, classNum, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # downsample\n",
    "        dlyr1 = self.dlyr1(x)\n",
    "        ds1 = self.ds(dlyr1) #\n",
    "\n",
    "        dlyr2 = self.dlyr2(ds1)\n",
    "        ds2 = self.ds(dlyr2)\n",
    "        dlyr3 = self.dlyr3(ds2)\n",
    "        ds3 = self.ds(dlyr3)\n",
    "        dlyr4 = self.dlyr4(ds3)\n",
    "        ds4 = self.ds(dlyr4)\n",
    "        dlyr5 = self.dlyr5(ds4)\n",
    "        ds_last = self.ds(dlyr5)\n",
    "        dlyr_last = self.dlyr6(ds_last)\n",
    "        # upsample\n",
    "\n",
    "        us_init = self.us_init(dlyr_last)\n",
    "        ulyr_init = self.ulyr_init(torch.cat([us_init, dlyr5], 1))\n",
    "        us6 = self.us6(ulyr_init)\n",
    "        merge6 = torch.cat([us6, dlyr4], 1)  # channel is the second dimension after batch operation\n",
    "        ulyr6 = self.ulyr6(merge6)\n",
    "        us7 = self.us7(ulyr6)\n",
    "        merge7 = torch.cat([us7, dlyr3], 1)\n",
    "        ulyr7 = self.ulyr7(merge7)\n",
    "        us8 = self.us8(ulyr7)\n",
    "        merge8 = torch.cat([us8, dlyr2], 1)\n",
    "        ulyr8=self.ulyr8(merge8)\n",
    "        us9 = self.us9(ulyr8)\n",
    "        merge9 = torch.cat([us9, dlyr1], 1)\n",
    "        ulyr9 = self.ulyr9(merge9)\n",
    "        dimTrans = self.dimTrans(ulyr9)\n",
    "        \n",
    "        return dimTrans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "h1GalIWv5jsk"
   },
   "source": [
    "### Training, Validatation and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true,
    "id": "KrwFxwce5jsk"
   },
   "outputs": [],
   "source": [
    "def train(trainData, model, criterion, optimizer, scheduler, trainLoss=[], gpu=True):\n",
    "    \"\"\"\n",
    "    Train model\n",
    "    Params:\n",
    "        trainData (''DataLoader''): Batch grouped data\n",
    "        model: Model to train\n",
    "        classNum (int): Number of categories to classify\n",
    "        criterion: Function to caculate loss\n",
    "        oprimizer: Funtion for optimzation\n",
    "        scheduler: Update policy for learning rate decay.\n",
    "        trainLoss: (empty list) To record average loss for each epoch\n",
    "        gpu: (binary,optional) Decide whether to use GPU, default is True\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # mini batch iteration\n",
    "    epoch_loss = 0\n",
    "    i = 0\n",
    "\n",
    "    for img, label, mask in trainData:\n",
    "\n",
    "        # forward\n",
    "        img = Variable(img)\n",
    "        label = Variable(label)\n",
    "        if gpu:\n",
    "            img = img.cuda()\n",
    "            label = label.cuda()\n",
    "\n",
    "        out = model(img)\n",
    "        label = label * mask.cuda()\n",
    "        mask = torch.stack([mask]*out.size()[1], dim=1)\n",
    "        out = out * mask.cuda()\n",
    "\n",
    "        loss = criterion()(out, label)\n",
    "        epoch_loss += loss.item()\n",
    "        i += 1\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # avoid calling to config.yaml\n",
    "        isCyclicLR = False\n",
    "        if type(scheduler) == torch.optim.lr_scheduler.CyclicLR:\n",
    "            scheduler.step()\n",
    "            isCyclicLR = True\n",
    "\n",
    "    print(f'train loss:{epoch_loss / i}')\n",
    "    if isCyclicLR:\n",
    "        print(f\"LR: {scheduler.get_last_lr()}\")\n",
    "\n",
    "    if trainLoss != None:\n",
    "        trainLoss.append(float(epoch_loss / i))\n",
    "\n",
    "##################################################\n",
    "\n",
    "def validate(valData, model, criterion, buffer, valLoss, gpu):\n",
    "    \"\"\"\n",
    "        Validate model\n",
    "        Params:\n",
    "            valData (''DataLoader''): Batch grouped data\n",
    "            model: Trained model for validation\n",
    "            criterion: Function to calculate loss\n",
    "            buffer: Buffer added to the targeted grid when creating dataset. \n",
    "                This allows loss to calculate at non-buffered region\n",
    "            valLoss (empty list): To record average loss for each epoch\n",
    "            gpu (binary,optional): Decide whether to use GPU, default is True\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # mini batch iteration\n",
    "    epoch_loss = 0\n",
    "    i = 0\n",
    "\n",
    "    for img, label in valData:\n",
    "\n",
    "        img = Variable(img, requires_grad=False)\n",
    "        label = Variable(label, requires_grad=False)\n",
    "\n",
    "        # GPU setting\n",
    "        if gpu:\n",
    "            img = img.cuda()\n",
    "            label = label.cuda()\n",
    "\n",
    "        out = model(img)\n",
    "\n",
    "        loss = criterion()(out[:, :, buffer:-buffer, buffer:-buffer],\n",
    "                            label[:, buffer:-buffer, buffer:-buffer])\n",
    "        epoch_loss += loss.item()\n",
    "        i += 1\n",
    "\n",
    "    print('validation loss: {}'.format(epoch_loss / i))\n",
    "\n",
    "    if valLoss != None:\n",
    "        valLoss.append(float(epoch_loss / i))\n",
    "\n",
    "##################################################\n",
    "\n",
    "def predict(predData, model, buffer, gpu, shrinkPixel):\n",
    "    \"\"\"\n",
    "    Predict by tile\n",
    "    Params:\n",
    "        predData (''DataLoader''): Batch grouped data\n",
    "        model: Trained model for prediction\n",
    "        buffer (int): Buffer to cut out when writing chips\n",
    "        gpu (binary,optional): Decide whether to use GPU, default is True\n",
    "        shrinkPixel (int, optional): pixel numbers to cut out on each side \n",
    "            before averging neighbors.\n",
    "    \"\"\"\n",
    "    predData, meta, tile = predData\n",
    "    meta.update({\n",
    "        'dtype': 'int8'\n",
    "    })\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # create dummy tile\n",
    "    canvas_score_ls = []\n",
    "\n",
    "    for img, index_batch in predData:\n",
    "\n",
    "        img = Variable(img, requires_grad=False)\n",
    "\n",
    "        # GPU setting\n",
    "        if gpu:\n",
    "            img = img.cuda()\n",
    "\n",
    "        out = F.softmax(model(img), 1)\n",
    "        batch, nclass, height, width = out.size()\n",
    "        chip_height = height - buffer * 2\n",
    "        chip_width = width - buffer * 2\n",
    "        max_index_0 = meta['height'] - chip_height\n",
    "        max_index_1 = meta['width'] - chip_width\n",
    "\n",
    "        # new by taking average\n",
    "        for i in range(batch):\n",
    "            index = (index_batch[0][i], index_batch[1][i])\n",
    "            # only score here\n",
    "            for n in range(nclass - 1):\n",
    "                out_score = out[\n",
    "                    :, n + 1, \n",
    "                    (index[0] != 0) * buffer : (index[0] != 0) * buffer + \\\n",
    "                    chip_height + (index[0]==0 or index[0] == max_index_0) * \\\n",
    "                    buffer,\n",
    "                    (index[1] != 0) * buffer: (index[1] != 0) * buffer + \\\n",
    "                    chip_height + (index[1] == 0 or index[1] == max_index_1) * \\\n",
    "                    buffer\n",
    "                ].data[i].cpu().numpy() * 100\n",
    "                out_score = out_score.astype(meta['dtype'])\n",
    "                score_height, score_width = out_score.shape\n",
    "\n",
    "                try:\n",
    "                    # if canvas_score_ls[n] exists\n",
    "                    canvas_score_ls[n][\n",
    "                        index[0] + buffer * (index[0] != 0): index[0] + \\\n",
    "                        buffer * (index[0] != 0)+ score_height,\n",
    "                        index[1]+ buffer * (index[1] != 0): index[1] + \\\n",
    "                        buffer * (index[1] != 0)+ score_width\n",
    "                    ] = out_score\n",
    "\n",
    "                except:\n",
    "                    # create masked canvas_score_ls[n]\n",
    "                    canvas_score = np.zeros(\n",
    "                        (meta['height'] + buffer * 2, \n",
    "                         meta['width'] + buffer * 2), dtype=meta['dtype']\n",
    "                    )\n",
    "\n",
    "                    canvas_score[\n",
    "                        index[0] + buffer * (index[0] != 0): index[0] + \\\n",
    "                        buffer * (index[0] != 0)+ score_height,\n",
    "                        index[1]+ buffer * (index[1] != 0): index[1] + \\\n",
    "                        buffer * (index[1] != 0)+ score_width\n",
    "                    ] = out_score\n",
    "                    canvas_score_ls.append(canvas_score)\n",
    "\n",
    "\n",
    "    for j in range(len(canvas_score_ls)):\n",
    "        canvas_score_ls[j] = canvas_score_ls[j][\n",
    "          shrinkPixel:meta['height'] + buffer * 2 -shrinkPixel, \n",
    "          shrinkPixel:meta['width'] + buffer * 2 - shrinkPixel\n",
    "        ]\n",
    "\n",
    "    return canvas_score_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "2YVVAp_Z5jsm"
   },
   "source": [
    "### Model Compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true,
    "id": "GqU0p0E-5jsm"
   },
   "outputs": [],
   "source": [
    "def get_optimizer(optimizer, params, lr, momentum):\n",
    "\n",
    "    optimizer = optimizer.lower()\n",
    "    if optimizer == 'sgd':\n",
    "        return torch.optim.SGD(params, lr, momentum=momentum)\n",
    "    elif optimizer == 'nesterov':\n",
    "        return torch.optim.SGD(params, lr, momentum=momentum, nesterov=True)\n",
    "    elif optimizer == 'adam':\n",
    "        return torch.optim.Adam(params, lr)\n",
    "    elif optimizer == 'amsgrad':\n",
    "        return torch.optim.Adam(params, lr, amsgrad=True)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"{} currently not supported,\" \\\n",
    "            \"please customize your optimizer in compiler.py\".format(optimizer)\n",
    "        )\n",
    "\n",
    "\n",
    "def weighted_average_overlay(predDict, overlayPixels):\n",
    "\n",
    "    if isinstance(predDict, dict):\n",
    "        key_ls = [\"top\", \"center\", \"left\", \"right\", \"bottom\"]\n",
    "        key_miss_ls = [m for m in predDict.keys() if m not in key_ls]\n",
    "        if len(key_miss_ls) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            assert \"Input must be dictionary containing data for centered image and its 4 neighbors.\"\\\n",
    "            \"Missed {}\".format(\", \".join(key_miss_ls))\n",
    "    else:\n",
    "        assert \"Input must be dictionary containing data for centered image and its 4 neighbors, \" \\\n",
    "               \"including including 'top', 'left', 'right', and  'bottom'\"\n",
    "\n",
    "    target = predDict['center']\n",
    "    h, w = target.shape\n",
    "    # top\n",
    "    if predDict['top'] is not None:\n",
    "        target_weight = np.array([1. / overlayPixels * np.arange(1, overlayPixels + 1)] * w).transpose(1, 0)\n",
    "        comp_weight = 1. - target_weight\n",
    "        # comp = scores_dict[\"up\"][- overlay_pixs : , : ]\n",
    "        target[:overlayPixels, :] = comp_weight * predDict['top'][- overlayPixels:, :] + \\\n",
    "                                   target_weight * target[:overlayPixels, :]\n",
    "    else:\n",
    "        pass\n",
    "    # bottom\n",
    "    if predDict['bottom'] is not None:\n",
    "        target_weight = np.array([1. / overlayPixels * np.flip(np.arange(1, overlayPixels + 1))] * w).transpose(1, 0)\n",
    "        comp_weight = 1. - target_weight\n",
    "        target[-overlayPixels:, :] = comp_weight * predDict['bottom'][:overlayPixels, :] + \\\n",
    "                                    target_weight * target[-overlayPixels:, :]\n",
    "    else:\n",
    "        pass\n",
    "    # left\n",
    "    if predDict['left'] is not None:\n",
    "        target_weight = np.array([1. / overlayPixels * np.arange(1, overlayPixels + 1)] * h)\n",
    "        comp_weight = 1 - target_weight\n",
    "        target[:, :overlayPixels] = comp_weight * predDict['left'][:, -overlayPixels:] + \\\n",
    "                                   target_weight * target[:, :overlayPixels]\n",
    "    else:\n",
    "        pass\n",
    "    # right\n",
    "    if predDict['right'] is not None:\n",
    "        target_weight = np.array([1. / overlayPixels * np.flip(np.arange(1, overlayPixels + 1))] * h)\n",
    "        comp_weight = 1 - target_weight\n",
    "        target[:, -overlayPixels:] = comp_weight * predDict['right'][:, :overlayPixels] + \\\n",
    "                                    target_weight * target[:, -overlayPixels:]\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "class ModelCompiler:\n",
    "    \"\"\"\n",
    "    Compiler of specified model\n",
    "    Args:\n",
    "        model (''nn.Module''): pytorch model for segmentation\n",
    "        buffer (int): distance to sample edges not considered in optimization\n",
    "        gpuDevices (list): indices of gpu devices to use\n",
    "        params_init (dict object): initial model parameters\n",
    "        freeze_params (list): list of indices for parameters to keep frozen\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, buffer, gpuDevices=[0], params_init=None, freeze_params=None):\n",
    "        \n",
    "        self.working_dir = config[\"working_dir\"]\n",
    "        self.out_dir = config[\"out_dir\"]\n",
    "\n",
    "        # model\n",
    "        self.gpuDevices = gpuDevices\n",
    "        self.model = model\n",
    "\n",
    "        self.model_name = self.model.__class__.__name__\n",
    "\n",
    "        if params_init:\n",
    "            self.load_params(params_init, freeze_params)\n",
    "\n",
    "        self.buffer = buffer\n",
    "\n",
    "        # gpu\n",
    "        self.gpu = torch.cuda.is_available()\n",
    "\n",
    "        if self.gpu:\n",
    "            print('----------GPU available----------')\n",
    "            # GPU setting\n",
    "            if gpuDevices:\n",
    "                torch.cuda.set_device(gpuDevices[0])\n",
    "                self.model = torch.nn.DataParallel(self.model, device_ids=gpuDevices)\n",
    "            self.model = self.model.cuda()\n",
    "\n",
    "        num_params = sum([p.numel() for p in self.model.parameters() if p.requires_grad])\n",
    "        print(\"total number of trainable parameters: {:2.1f}M\".format(num_params / 1000000))\n",
    "\n",
    "        if params_init:\n",
    "            print(\"---------- Pre-trained model compiled successfully ----------\")\n",
    "        else:\n",
    "            print(\"---------- Vanilla Model compiled successfully ----------\")\n",
    "\n",
    "    def load_params(self, dir_params, freeze_params):\n",
    "\n",
    "        params_init = urlparse.urlparse(dir_params)\n",
    "\n",
    "        inparams = torch.load(params_init.path)\n",
    "\n",
    "        ## overwrite model entries with new parameters\n",
    "        model_dict = self.model.state_dict()\n",
    "\n",
    "        if \"module\" in list(inparams.keys())[0]:\n",
    "            inparams_filter = {k[7:]: v.cpu() for k, v in inparams.items() if k[7:] in model_dict}\n",
    "\n",
    "        else:\n",
    "            inparams_filter = {k: v.cpu() for k, v in inparams.items() if k in model_dict}\n",
    "        model_dict.update(inparams_filter)\n",
    "        # load new state dict\n",
    "        self.model.load_state_dict(model_dict)\n",
    "\n",
    "        # free some layers\n",
    "        if freeze_params != None:\n",
    "            for i, p in enumerate(self.model.parameters()):\n",
    "                if i in freeze_params:\n",
    "                    p.requires_grad = False\n",
    "\n",
    "\n",
    "    def fit(self, trainDataset, valDataset, epochs, optimizer_name, lr_init, lr_policy,\n",
    "            criterion, momentum = None, resume=False, resume_epoch=None, **kwargs):\n",
    "        \n",
    "        self.model_dir = \"{}/{}/{}_ep{}\".format(self.working_dir, self.out_dir, config[\"model_name\"], config[\"epochs\"])\n",
    "        \n",
    "        if not os.path.exists(Path(self.working_dir) / self.out_dir / self.model_dir):\n",
    "            os.makedirs(Path(self.working_dir) / self.out_dir / self.model_dir)\n",
    "        \n",
    "        self.checkpoint_dir = Path(self.working_dir) / self.out_dir / self.model_dir / \"chkpt\"\n",
    "        if not os.path.exists(self.checkpoint_dir):\n",
    "            os.makedirs(self.checkpoint_dir)\n",
    "        \n",
    "        os.chdir(Path(self.working_dir) / self.out_dir / self.model_dir)\n",
    "        \n",
    "        print(\"-------------------------- Start training --------------------------\")\n",
    "        start = datetime.now()\n",
    "        \n",
    "        writer = SummaryWriter('./')\n",
    "        lr = lr_init\n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "        \n",
    "        optimizer = get_optimizer(optimizer_name, \n",
    "                                  filter(lambda p: p.requires_grad, self.model.parameters()),\n",
    "                                  lr, \n",
    "                                  momentum)\n",
    "\n",
    "        # initialize different learning rate scheduler\n",
    "        lr_policy = lr_policy.lower()\n",
    "        if lr_policy == \"StepLR\".lower():\n",
    "            step_size = kwargs.get(\"step_size\", 3)\n",
    "            gamma = kwargs.get(\"gamma\", 0.98)\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "                optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "        elif lr_policy == \"MultiStepLR\".lower():\n",
    "            milestones = kwargs.get(\"milestones\", [15, 25, 35, 50, 70, 90, 120, 150, 200])\n",
    "            gamma = kwargs.get(\"gamma\", 0.5)\n",
    "            scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "                optimizer, milestones=milestones, gamma=gamma,\n",
    "            )\n",
    "\n",
    "        elif lr_policy == \"ReduceLROnPlateau\".lower():\n",
    "            mode = kwargs.get('mode', 'min')\n",
    "            factor = kwargs.get('factor', 0.8)\n",
    "            patience = kwargs.get('patience', 3)\n",
    "            threshold = kwargs.get('threshold', 0.0001)\n",
    "            threshold_mode = kwargs.get('threshold_mode', 'rel')\n",
    "            min_lr = kwargs.get('min_lr', 3e-6)\n",
    "            verbose = kwargs.get('verbose', True)\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, mode=mode, factor=factor, patience=patience, threshold=threshold,\n",
    "                threshold_mode=threshold_mode, min_lr=min_lr, verbose=verbose\n",
    "            )\n",
    "\n",
    "        elif lr_policy == \"PolynomialLR\".lower():\n",
    "            max_decay_steps = kwargs.get('max_decay_steps', 100)\n",
    "            min_learning_rate = kwargs.get('min_learning_rate', 1e-5)\n",
    "            power = kwargs.get('power', 0.8)\n",
    "            scheduler = PolynomialLR(\n",
    "                optimizer, max_decay_steps=max_decay_steps, min_learning_rate=min_learning_rate,\n",
    "                power=power\n",
    "            )\n",
    "\n",
    "        elif lr_policy == \"CyclicLR\".lower():\n",
    "            base_lr = kwargs.get('base_lr', 3e-5)\n",
    "            max_lr = kwargs.get('max_lr', 0.01)\n",
    "            step_size_up = kwargs.get('step_size_up', 1100)\n",
    "            mode =  kwargs.get('mode', 'triangular')\n",
    "            scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "                optimizer, base_lr=base_lr, max_lr=max_lr, step_size_up=step_size_up,\n",
    "                mode=mode\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            scheduler = None\n",
    "\n",
    "        if resume:\n",
    "            model_state_file = os.path.join(\n",
    "                self.checkpoint_dir,\n",
    "                f\"{resume_epoch}_checkpoint.pth.tar\"\n",
    "            )\n",
    "\n",
    "            # Resume the model from the specified checkpoint in the config file.\n",
    "            if os.path.exists(model_state_file):\n",
    "                print(f\"Model is resumed from checkpoint: {model_state_file}\")\n",
    "                checkpoint = torch.load(model_state_file)\n",
    "                resume_epoch = checkpoint[\"epoch\"]\n",
    "                scheduler.load_state_dict(checkpoint[\"scheduler\"])\n",
    "                self.model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "                optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "                train_loss = checkpoint[\"train_loss\"]\n",
    "                val_loss = checkpoint[\"val_loss\"]\n",
    "            else:\n",
    "                raise ValueError(f\"{model_state_file} does not exist\")\n",
    "\n",
    "        if resume:\n",
    "            iterable = range(resume_epoch, epochs)\n",
    "        else:\n",
    "            iterable = range(epochs)\n",
    "\n",
    "        for t in iterable:\n",
    "\n",
    "            print(f\"[{t+1}/{epochs}]\")\n",
    "\n",
    "            # start fitting\n",
    "            start_epoch = datetime.now()\n",
    "            train(trainDataset, self.model, criterion, optimizer, scheduler, \n",
    "                  gpu=self.gpu, trainLoss=train_loss)\n",
    "            validate(valDataset, self.model, criterion, self.buffer, \n",
    "                     gpu=self.gpu, valLoss=val_loss)\n",
    "\n",
    "            # Update the scheduler\n",
    "            if lr_policy in [\"StepLR\".lower(), \"MultiStepLR\".lower()]:\n",
    "                scheduler.step()\n",
    "                print(f\"LR: {scheduler.get_last_lr()}\")\n",
    "\n",
    "            if lr_policy == \"ReduceLROnPlateau\".lower():\n",
    "                scheduler.step(val_loss[t])\n",
    "\n",
    "            if lr_policy == \"PolynomialLR\".lower():\n",
    "                scheduler.step(t)\n",
    "                print(f\"LR: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "            # time spent on single iteration\n",
    "            print('time:', (datetime.now() - start_epoch).seconds)\n",
    "\n",
    "            writer.add_scalars(\n",
    "                \"Loss\",\n",
    "                {\"train_loss\": train_loss[t],\n",
    "                 \"val_loss\": val_loss[t]},\n",
    "                 t + 1)\n",
    "\n",
    "            checkpoint_interval = 2 # e.g. save every 10 epochs\n",
    "            if (t+1) % checkpoint_interval == 0:\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"epoch\": t+1,\n",
    "                        \"state_dict\": self.model.state_dict(),\n",
    "                        \"scheduler\": scheduler.state_dict(),\n",
    "                        \"optimizer\": optimizer.state_dict(),\n",
    "                        \"train_loss\": train_loss,\n",
    "                        \"val_loss\": val_loss\n",
    "                    }, os.path.join(\n",
    "                        self.checkpoint_dir,\n",
    "                        f\"{t+1}_checkpoint.pth.tar\")\n",
    "                )\n",
    "\n",
    "        writer.close()\n",
    "\n",
    "        print(f\"-------------------------- Training finished in {(datetime.now() - start).seconds}s --------------------------\")\n",
    "\n",
    "\n",
    "    def evaluate(self, evalDataset, csv_fn):\n",
    "\n",
    "        if not os.path.exists(Path(self.working_dir) / self.out_dir):\n",
    "            os.makedirs(Path(self.working_dir) / self.out_dir)\n",
    "        \n",
    "        print('-------------------------- Start evaluation --------------------------')\n",
    "        start = datetime.now()\n",
    "\n",
    "        evaluate(evalDataset, self.model, self.buffer, csv_fn, self.gpu)\n",
    "\n",
    "        print(f\"-------------------------- Evaluation finished in {(datetime.now() - start).seconds}s --------------------------\")\n",
    "\n",
    "\n",
    "    def predict(self, predDataset, out_prefix, predBuffer=None, \n",
    "                averageNeighbors=False, shrinkBuffer=0):\n",
    "        \n",
    "        # predDataset must be dictionary containing target and all 4 neighbors if averageNeighbors\n",
    "        if averageNeighbors == True:\n",
    "            if isinstance(predDataset, dict):\n",
    "                key_ls = [\"top\", \"center\", \"left\", \"right\", \"bottom\"]\n",
    "                key_miss_ls = [m for m in predDataset.keys() if m not in key_ls]\n",
    "                if len(key_miss_ls) == 0:\n",
    "                    pass\n",
    "                else:\n",
    "                    assert \"predDataset must be dictionary containing data for centered image and its 4 neighbors when \" \\\n",
    "                           \"averageNeighbors set to be True. Missed {}\".format(\", \".join(key_miss_ls))\n",
    "            else:\n",
    "                assert \"predDataset must be dictionary containing data for centered image and its 4 neighbors when \" \\\n",
    "                       \"averageNeighbors set to be True, including 'top', 'left', 'right', 'bottom'\"\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        print('-------------------------- Start prediction --------------------------')\n",
    "        start = datetime.now()\n",
    "        \n",
    "        if out_prefix is None:\n",
    "            out_prefix = Path(self.working_dir) / self.out_dir / \"Inference_output\"\n",
    "            Path(out_prefix).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        _, meta, tile = predDataset[\"center\"] if isinstance(predDataset, dict) else predDataset\n",
    "        #set_trace()\n",
    "        name_score = 'score_c{}_r{}.tif'.format(tile[0], tile[1])\n",
    "        meta.update({\n",
    "            'dtype': 'int8'\n",
    "        })\n",
    "\n",
    "        new_buffer = predBuffer - shrinkBuffer\n",
    "        \n",
    "        if averageNeighbors:\n",
    "            scores_dict = {k: predict(predDataset[k], self.model, predBuffer, gpu=self.gpu, shrinkPixel=shrinkBuffer) if predDataset[k]\n",
    "                           else None for k in predDataset.keys()}\n",
    "\n",
    "            nclass = len(list(scores_dict['center']))\n",
    "            overlay_pixs = new_buffer * 2\n",
    "\n",
    "            for n in range(nclass):\n",
    "                score_dict = {k: scores_dict[k][n] if scores_dict[k] else None for k in scores_dict.keys()}\n",
    "                score = weighted_average_overlay(score_dict, overlay_pixs)\n",
    "                # write to Drive\n",
    "                score = score[new_buffer: meta['height'] + new_buffer, new_buffer:meta['height'] + new_buffer]\n",
    "                score = np.expand_dims(score, axis=0).astype(meta['dtype'])\n",
    "                \n",
    "                updated_name_score = \"class_{}_\".format(n) + name_score\n",
    "                with rasterio.open(Path(out_prefix) / updated_name_score, \"w\", **meta) as dst:\n",
    "                    dst.write(score)\n",
    "                    \n",
    "\n",
    "        # when not averageNeighbors\n",
    "        else:\n",
    "            scores = predict(predDataset, self.model, predBuffer, gpu=self.gpu, shrinkPixel=shrinkBuffer)\n",
    "            # write score of each non-background classes into s3\n",
    "            nclass = len(scores)\n",
    "            for n in range(nclass):\n",
    "                canvas = scores[n][new_buffer: meta['height'] + new_buffer, new_buffer: meta['width'] + new_buffer]\n",
    "                canvas = np.expand_dims(canvas, axis=0).astype(meta['dtype'])\n",
    "\n",
    "                updated_name_score = \"class_{}_\".format(n) + name_score\n",
    "                with rasterio.open(Path(out_prefix) / updated_name_score, \"w\", **meta) as dst:\n",
    "                    dst.write(canvas)\n",
    "\n",
    "        print('-------------------------- Prediction finished in {}s --------------------------' \\\n",
    "              .format((datetime.now() - start).seconds))\n",
    "    \n",
    "    def save(self, object = \"params\"):\n",
    "        \n",
    "        if object == \"params\":\n",
    "            torch.save(self.model.state_dict(), \n",
    "                       os.path.join(self.checkpoint_dir, \"{}_final_state.pth\".format(config[\"model_name\"])))\n",
    "            \n",
    "            print(\"--------------------- Model parameters is saved to disk ---------------------\")\n",
    "        \n",
    "        elif object == \"model\":\n",
    "            torch.save(self.model, \n",
    "                       os.path.join(self.checkpoint_dir, \"{}_final_state.pth\".format(config[\"model_name\"])))\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Improper object type.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqimzk8w5jsn"
   },
   "source": [
    "## Train the Model\n",
    "\n",
    "### Define parameters\n",
    "\n",
    "We first set up a configuration dictionary with all the parameters the model needs to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rm_gxzCQ83Yh",
    "outputId": "bb00c19a-1f73-4df7-e0a1-85c740fd4aeb"
   },
   "outputs": [],
   "source": [
    "#!ls /content/gdrive/MyDrive/teaching/geog287387/data/fieldmapping/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cF_fmejT5jso"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \n",
    "    #I/O setup\n",
    "    \"working_dir\" : \"/content/gdrive/MyDrive/working_folder\",\n",
    "    \"out_dir\" : \"testing_11\",\n",
    "    \n",
    "    \"root_dir\" : \"/content/gdrive/MyDrive/field_mapping/\",\n",
    "    \"catalog_train_fn\" : \"catalog_ghana_ecaas_ejura_tain.csv\",\n",
    "    \"catalog_pred_fn\" : \"catalog_predict_nicfi_retiled_ejura_tain_2020-11.csv\",\n",
    "    \n",
    "    # Train Dataset and Loader\n",
    "    \"patch_size\" : 200,\n",
    "    \"buffer\" : 12,\n",
    "    \"composite_buffer\" : 11,\n",
    "    \"img_path_cols\": ['dir_gs', 'dir_os'],\n",
    "    \"label_path_col\": \"dir_label\",\n",
    "    \"label_group_train\" : [2, 3, 4],\n",
    "    \"transformation\" : ['vflip', 'hflip', 'rotate', 'resize', 'shift_brightness'],\n",
    "    \"rotate_degree\" : [-90, 90],\n",
    "    \"brightness_shift_subsets\": [4, 4],\n",
    "    \"train_batch\" : 12,\n",
    "    \n",
    "    # Validation Dataset and Loader\n",
    "    \"label_grou_val\": [3, 4],\n",
    "    \"val_batch\": 1,\n",
    "    \n",
    "    # Model\n",
    "    \"model_name\" : \"Unet\",\n",
    "    \"img_bands\" : 8,\n",
    "    \"class_numbers\" : 3,\n",
    "    \n",
    "    # Compiler\n",
    "    \"gpus\" : [0],\n",
    "    \"init_params\" : \"unet_params.pth\",\n",
    "    \"freeze_params\": list(range(58)),\n",
    "    #\"init_params\" : \n",
    "    #    \"/content/gdrive/MyDrive/working_folder/params/Unet_ep5/\"\\\n",
    "    #    \"chkpt/Unet_final_state.pth\",    \n",
    "    \n",
    "    # Model fitting\n",
    "    \"epochs\" : 5,\n",
    "    \"optimizer\" : 'nesterov',\n",
    "    \"momentum\" : 0.95,\n",
    "    \"lr_init\" : 0.01,\n",
    "    \"LR_policy\" : \"StepLR\",\n",
    "    \"criterion\" :  BalancedTverskyFocalLoss(gamma = 0.9),\n",
    "    \"resume\" : False,\n",
    "    \"resume_epoch\" : None,\n",
    "    \n",
    "    # Evaluation report on validation dataset\n",
    "    \"val_metric_fname\" : \"validate_metrics.csv\",\n",
    "    \n",
    "    # Prediction (Inference)\n",
    "    \"patch_size_pred\" : 250,\n",
    "    \"buffer_pred\" : 179,\n",
    "    \"composite_buffer_pred\" : 179,\n",
    "    \"batch_pred\" : 2,\n",
    "    \"average_neighbors\": False,\n",
    "    \"shrink_pixels\": 54,\n",
    "\n",
    "    \"out_prefix\": None  \n",
    "}\n",
    "\n",
    "if not os.path.exists(config[\"working_dir\"]):\n",
    "    os.makedirs(config[\"working_dir\"])\n",
    "os.chdir(config[\"working_dir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in training data \n",
    "\n",
    "Load the catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "jeEZ1GAk5jss"
   },
   "outputs": [],
   "source": [
    "# Reading the train csv\n",
    "train_catalog = pd.read_csv(os.path.join(config[\"root_dir\"], config[\"catalog_train_fn\"]))\n",
    "train_catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the dataloader, which takes a few minutes. This compiles the training dataset consisting of PlanetScope basemap imagery and labels. It chips up the imagery, pairs the chips with labels, puts the sample pairs into mini-batches, performs augmentations, and load them onto the GPU to be fed into the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bzMWcLUH5jst",
    "outputId": "6254a69e-f6f1-44c6-c26f-dd628489098e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = planetData(root_dir=config[\"root_dir\"], \n",
    "                        catalog=train_catalog, \n",
    "                        dataSize=config[\"patch_size\"], \n",
    "                        buffer=config[\"buffer\"], \n",
    "                        bufferComp=config[\"composite_buffer\"], \n",
    "                        usage=\"train\",\n",
    "                        imgPathCols=config[\"img_path_cols\"] ,\n",
    "                        labelPathCol=config[\"label_path_col\"],\n",
    "                        labelGroup=config[\"label_group_train\"], \n",
    "                        deRotate=config[\"rotate_degree\"], \n",
    "                        bShiftSubs=config[\"brightness_shift_subsets\"],\n",
    "                        trans=config[\"transformation\"])\n",
    "\n",
    "train_dataloader = DataLoader(train_data, \n",
    "                              batch_size=config[\"train_batch\"], \n",
    "                              shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then do the same for the validation sample..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vW1PmZn85jst",
    "outputId": "2a0e8e68-5ff5-4cb6-9945-3dadadce9907"
   },
   "outputs": [],
   "source": [
    "validate_data = planetData(\n",
    "    root_dir=config[\"root_dir\"], \n",
    "    catalog=train_catalog, \n",
    "    dataSize=config[\"patch_size\"], \n",
    "    buffer=config[\"buffer\"], \n",
    "    bufferComp=config[\"composite_buffer\"], \n",
    "    usage=\"validate\",\n",
    "    imgPathCols=config[\"img_path_cols\"],\n",
    "    labelPathCol=config[\"label_path_col\"],\n",
    "    labelGroup=config[\"label_grou_val\"]\n",
    ")\n",
    "\n",
    "validate_dataloader = DataLoader(\n",
    "    validate_data, \n",
    "    batch_size=config[\"val_batch\"], \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and compile the model\n",
    "\n",
    "Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "B1GbX8rH5jst"
   },
   "outputs": [],
   "source": [
    "model = eval(config[\"model_name\"].lower())(config[\"img_bands\"], config[\"class_numbers\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mt57WtFf5jsu",
    "outputId": "1c5f212f-b8c2-468c-bd93-6893c08bd4b0"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = ModelCompiler(\n",
    "    model=model, \n",
    "    buffer=config[\"buffer\"], \n",
    "    gpuDevices=config[\"gpus\"], \n",
    "    params_init=f'{config[\"root_dir\"]}{config[\"init_params\"]}',\n",
    "    freeze_params=config[\"freeze_params\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l2G2tcLH5jsu",
    "outputId": "263dcb8f-0798-4e1b-d89c-dabf15b0a729"
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    trainDataset=train_dataloader,\n",
    "    valDataset=validate_dataloader, \n",
    "    epochs=config[\"epochs\"],\n",
    "    optimizer_name=config[\"optimizer\"], \n",
    "    lr_init=config[\"lr_init\"], \n",
    "    lr_policy=config[\"LR_policy\"],\n",
    "    criterion=config[\"criterion\"], \n",
    "    momentum = config[\"momentum\"], \n",
    "    resume=config[\"resume\"], \n",
    "    resume_epoch=config[\"resume_epoch\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained model parameters for further use either fine-tunning or making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UlLm4JUO5jsu",
    "outputId": "c616fb95-f0d1-42b5-c3bf-9d5e476fa944",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save(object=\"params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model performance\n",
    "\n",
    "Against the validation dataset, saving the report as a csv file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n5rKoSEj5jsu",
    "outputId": "3c451ab2-7722-4223-c5d7-87934070adf3"
   },
   "outputs": [],
   "source": [
    "os.chdir(Path(config[\"working_dir\"])/config[\"out_dir\"])\n",
    "model.evaluate(validate_dataloader, csv_fn = config[\"val_metric_fname\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-E3s9OPu5jsu",
    "outputId": "95a95995-f461-41e3-c7fd-3a964862c331"
   },
   "outputs": [],
   "source": [
    "!cp validate_metrics.csv \"/content/gdrive/MyDrive/working_folder/testing_11/Unet_ep5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Prediction Maps\n",
    "\n",
    "Using the trained model, we will predict cropland locations on several PlanetScope image tiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXpGErMaDyu6"
   },
   "source": [
    "### Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "jfopeVXe5jsu"
   },
   "outputs": [],
   "source": [
    "# Tile reader: goes through each row of the prediction csv (e.g. each tile) complete the prediction, \n",
    "# write the output (both probability and harden) and move to the next tile.\n",
    "def load_pred_data(dir_data, pred_patch_size, pred_buffer, pred_composite_buffer, \n",
    "                   pred_batch, catalog, catalog_row, img_path_cols, average_neighbors=False):\n",
    "    def load_single_tile(catalog_ind = catalog_row):\n",
    "        dataset = planetData(dir_data, catalog, pred_patch_size, pred_buffer, \n",
    "                             pred_composite_buffer, \"predict\", \n",
    "                             catalogIndex=catalog_ind, imgPathCols=img_path_cols)\n",
    "        data_loader = DataLoader(dataset, batch_size=pred_batch, shuffle=False)\n",
    "        meta = dataset.meta\n",
    "        tile = dataset.tile\n",
    "        return data_loader, meta, tile\n",
    "\n",
    "    if average_neighbors == True:\n",
    "        catalog[\"tile_col_row\"] = catalog.apply(lambda x: \"{}_{}\".format(x['tile_col'], x['tile_row']), axis=1)\n",
    "        tile_col = catalog.iloc[catalog_row].tile_col\n",
    "        tile_row = catalog.iloc[catalog_row].tile_row\n",
    "        row_dict = {\n",
    "            \"center\": catalog_row,\n",
    "            \"top\": catalog.query('tile_col=={} & tile_row=={}'.format(tile_col, tile_row - 1)).iloc[0].name \\\n",
    "                if \"{}_{}\".format(tile_col, tile_row - 1) in list(catalog.tile_col_row) else None,\n",
    "            \"left\" : catalog.query('tile_col=={} & tile_row=={}'.format(tile_col - 1, tile_row)).iloc[0].name \\\n",
    "                if \"{}_{}\".format(tile_col - 1, tile_row) in list(catalog.tile_col_row) else None,\n",
    "            \"right\" : catalog.query('tile_col=={} & tile_row=={}'.format(tile_col + 1, tile_row)).iloc[0].name \\\n",
    "                if \"{}_{}\".format(tile_col + 1, tile_row) in list(catalog.tile_col_row) else None,\n",
    "            \"bottom\": catalog.query('tile_col=={} & tile_row=={}'.format(tile_col, tile_row + 1)).iloc[0].name \\\n",
    "                if \"{}_{}\".format(tile_col, tile_row + 1) in list(catalog.tile_col_row) else None,\n",
    "            }\n",
    "        dataset_dict = {k:load_single_tile(catalog_ind = row_dict[k]) if row_dict[k] is not None else None \n",
    "                        for k in row_dict.keys()}\n",
    "        return dataset_dict\n",
    "    # direct crop edge pixels\n",
    "    else:\n",
    "        return load_single_tile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read prediction catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-_hNFz4Z5jsu"
   },
   "outputs": [],
   "source": [
    "pred_catalog = pd.read_csv(\n",
    "    os.path.join(config[\"root_dir\"], config[\"catalog_pred_fn\"])\n",
    ")\n",
    "\n",
    "# make list of tile indices to query\n",
    "inds = pred_catalog.query(\"type == 'center'\").index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "wWec2Dsp5jsv",
    "outputId": "191e3a59-b3c2-455a-d380-ca5f58f6ab25"
   },
   "source": [
    "for i in inds:\n",
    "    print(\"Predicting on index %s\" % (i))\n",
    "    pred_dataloader = load_pred_data(\n",
    "        config['root_dir'], config['patch_size_pred'], config['buffer_pred'], \n",
    "        config[\"composite_buffer_pred\"], config['batch_pred'], \n",
    "        pred_catalog, i, config['img_path_cols'], \n",
    "        average_neighbors = config['average_neighbors']\n",
    "    )\n",
    "    p = model.predict(\n",
    "        pred_dataloader, config[\"out_prefix\"], config['buffer_pred'], \n",
    "        averageNeighbors=config['average_neighbors'], \n",
    "        shrinkBuffer=config['shrink_pixels']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot predictions\n",
    "\n",
    "Load in predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_path = f'{config[\"working_dir\"]}/{config[\"out_dir\"]}/Inference_output'\n",
    "preds = [f'{pred_path}/{file}' for file in os.listdir(pred_path)]\n",
    "score_maps = [rasterio.open(p) for p in preds]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "field_mapper.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
